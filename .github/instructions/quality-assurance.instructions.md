---
applyTo: "**/*quality*,**/*validation*,**/*testing*,**/*qa*"
description: "Data quality frameworks, validation, testing protocols"
---

# Quality Assurance Procedural Memory

## Data Quality Framework
- Implement comprehensive data quality dimensions (accuracy, completeness, consistency, timeliness, validity, uniqueness)
- Create data quality rules and business logic validation
- Establish data quality scorecards and KPIs
- Implement automated data quality monitoring and alerting
- Create data quality remediation workflows and escalation procedures

## Validation and Testing Protocols
- Implement unit tests for all data transformation logic
- Create integration tests for end-to-end data pipelines
- Establish data contract testing between systems
- Implement regression testing for data changes
- Create performance testing for large-scale data operations

## Automated Quality Monitoring
- Use statistical process control for data quality monitoring
- Implement anomaly detection for data distribution changes
- Create data freshness and completeness monitoring
- Establish data lineage validation and impact analysis
- Monitor data quality trends and degradation patterns

## Quality Assurance Processes
- Create data quality assessment and profiling procedures
- Establish code review processes for data analysis scripts
- Implement peer review workflows for analytical outputs
- Create documentation standards and review processes
- Establish quality gates for data pipeline deployments

## Continuous Improvement
- Conduct regular data quality assessments and gap analysis
- Implement feedback loops from data consumers
- Create data quality improvement initiatives and tracking
- Establish quality metrics benchmarking and comparison
- Conduct root cause analysis for data quality issues
