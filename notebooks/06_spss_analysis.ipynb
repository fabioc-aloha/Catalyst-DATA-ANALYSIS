{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65defef6",
   "metadata": {},
   "source": [
    "# 📊 SPSS Data Analysis: DBA 710 Multiple Stores\n",
    "\n",
    "## Overview\n",
    "This notebook provides comprehensive analysis of the **DBA 710 Multiple Stores.sav** SPSS dataset, including:\n",
    "- 🔍 **Schema Exploration**: Data structure, variable types, and metadata\n",
    "- 📈 **Descriptive Statistics**: Central tendency, dispersion, and distribution analysis\n",
    "- 🔗 **Correlation Analysis**: Relationships between variables and multivariate patterns\n",
    "- 📋 **Data Quality Assessment**: Missing values, outliers, and data integrity\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26607468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential Libraries for SPSS Analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyreadstat\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import itertools\n",
    "\n",
    "# Configure display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "# Configure visualization settings\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Enterprise color palette\n",
    "enterprise_colors = {\n",
    "    'primary': '#1f77b4',\n",
    "    'secondary': '#ff7f0e', \n",
    "    'success': '#2ca02c',\n",
    "    'warning': '#d62728',\n",
    "    'info': '#9467bd',\n",
    "    'accent': '#8c564b'\n",
    "}\n",
    "\n",
    "print(\"📚 Libraries loaded successfully!\")\n",
    "print(f\"🐼 Pandas version: {pd.__version__}\")\n",
    "print(f\"📊 NumPy version: {np.__version__}\")\n",
    "print(f\"📈 Matplotlib version: {plt.matplotlib.__version__}\")\n",
    "print(\"✅ Ready for SPSS data analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bf7e25",
   "metadata": {},
   "source": [
    "## 📂 Data Loading & Initial Inspection\n",
    "\n",
    "Let's load the SPSS file and examine its basic structure and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4936bdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SPSS file with metadata\n",
    "print(\"🔄 Loading SPSS file: DBA 710 Multiple Stores.sav\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    # Read SPSS file with pyreadstat to preserve metadata\n",
    "    df, meta = pyreadstat.read_sav(\n",
    "        'notebooks/DBA 710 Multiple Stores.sav',\n",
    "        apply_value_formats=True,  # Apply SPSS value labels\n",
    "        formats_as_ordered_category=True  # Preserve ordered categories\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Successfully loaded SPSS file!\")\n",
    "    print(f\"📊 Dataset shape: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "    print(f\"💾 Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"❌ File not found. Please ensure 'DBA 710 Multiple Stores.sav' is in the notebooks/ directory\")\n",
    "    print(\"📁 Current working directory contents:\")\n",
    "    import os\n",
    "    for item in os.listdir('notebooks/'):\n",
    "        if item.endswith('.sav'):\n",
    "            print(f\"   📄 {item}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading file: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e27184c",
   "metadata": {},
   "source": [
    "## 🔍 Comprehensive Schema Analysis\n",
    "\n",
    "Detailed exploration of the dataset structure, variable types, and SPSS metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414c37a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comprehensive schema information\n",
    "print(\"🔍 COMPREHENSIVE SCHEMA ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Basic dataset info\n",
    "print(f\"📊 Dataset Overview:\")\n",
    "print(f\"   • Total observations: {df.shape[0]:,}\")\n",
    "print(f\"   • Total variables: {df.shape[1]}\")\n",
    "print(f\"   • Missing values: {df.isnull().sum().sum():,} ({df.isnull().sum().sum() / (df.shape[0] * df.shape[1]) * 100:.2f}%)\")\n",
    "print(f\"   • Complete cases: {df.dropna().shape[0]:,} ({df.dropna().shape[0] / df.shape[0] * 100:.2f}%)\")\n",
    "\n",
    "print(f\"\\n📋 Variable Information:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Create comprehensive variable summary\n",
    "variable_summary = []\n",
    "\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    col_info = {\n",
    "        'Position': i,\n",
    "        'Variable': col,\n",
    "        'Type': str(df[col].dtype),\n",
    "        'Non_Null': df[col].count(),\n",
    "        'Null_Count': df[col].isnull().sum(),\n",
    "        'Null_Percent': f\"{df[col].isnull().sum() / len(df) * 100:.1f}%\",\n",
    "        'Unique_Values': df[col].nunique(),\n",
    "        'Memory_MB': f\"{df[col].memory_usage(deep=True) / 1024**2:.3f}\"\n",
    "    }\n",
    "    \n",
    "    # Add variable label if available from SPSS metadata\n",
    "    if hasattr(meta, 'column_names_to_labels') and col in meta.column_names_to_labels:\n",
    "        col_info['SPSS_Label'] = meta.column_names_to_labels[col]\n",
    "    else:\n",
    "        col_info['SPSS_Label'] = 'No label'\n",
    "    \n",
    "    # Add sample values for categorical/string variables\n",
    "    if df[col].dtype == 'object' or df[col].dtype.name == 'category':\n",
    "        unique_vals = df[col].dropna().unique()[:5]  # First 5 unique values\n",
    "        col_info['Sample_Values'] = ', '.join([str(val) for val in unique_vals])\n",
    "        if len(unique_vals) == 5 and df[col].nunique() > 5:\n",
    "            col_info['Sample_Values'] += ', ...'\n",
    "    else:\n",
    "        # For numeric variables, show range\n",
    "        if df[col].count() > 0:\n",
    "            col_info['Sample_Values'] = f\"Range: {df[col].min():.3f} to {df[col].max():.3f}\"\n",
    "        else:\n",
    "            col_info['Sample_Values'] = \"All missing\"\n",
    "    \n",
    "    variable_summary.append(col_info)\n",
    "\n",
    "# Convert to DataFrame for better display\n",
    "schema_df = pd.DataFrame(variable_summary)\n",
    "\n",
    "# Display the schema table\n",
    "print(schema_df.to_string(index=False))\n",
    "\n",
    "# Data type summary\n",
    "print(f\"\\n📊 Data Type Distribution:\")\n",
    "print(\"-\" * 30)\n",
    "dtype_counts = df.dtypes.value_counts()\n",
    "for dtype, count in dtype_counts.items():\n",
    "    print(f\"   • {dtype}: {count} variables ({count/len(df.columns)*100:.1f}%)\")\n",
    "\n",
    "# SPSS Metadata Information\n",
    "if hasattr(meta, 'file_encoding'):\n",
    "    print(f\"\\n🔤 SPSS File Information:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"   • File encoding: {meta.file_encoding}\")\n",
    "    if hasattr(meta, 'creation_time'):\n",
    "        print(f\"   • Creation time: {meta.creation_time}\")\n",
    "    if hasattr(meta, 'modification_time'):\n",
    "        print(f\"   • Last modified: {meta.modification_time}\")\n",
    "\n",
    "# Value labels (SPSS factor levels)\n",
    "if hasattr(meta, 'value_labels') and meta.value_labels:\n",
    "    print(f\"\\n🏷️ SPSS Value Labels (Categorical Variables):\")\n",
    "    print(\"-\" * 30)\n",
    "    for var, labels in meta.value_labels.items():\n",
    "        if var in df.columns:\n",
    "            print(f\"   • {var}:\")\n",
    "            for value, label in list(labels.items())[:5]:  # Show first 5 labels\n",
    "                print(f\"     {value}: {label}\")\n",
    "            if len(labels) > 5:\n",
    "                print(f\"     ... and {len(labels) - 5} more labels\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eab0c96",
   "metadata": {},
   "source": [
    "## 📈 Comprehensive Descriptive Statistics\n",
    "\n",
    "Detailed statistical analysis for all variables, including measures of central tendency, dispersion, and distribution shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094ae103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive descriptive statistics\n",
    "print(\"📈 COMPREHENSIVE DESCRIPTIVE STATISTICS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Separate numeric and categorical variables\n",
    "numeric_vars = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_vars = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"🔢 Numeric Variables: {len(numeric_vars)}\")\n",
    "print(f\"🏷️ Categorical Variables: {len(categorical_vars)}\")\n",
    "print()\n",
    "\n",
    "# Enhanced descriptive statistics for numeric variables\n",
    "if numeric_vars:\n",
    "    print(\"🔢 NUMERIC VARIABLES - DETAILED STATISTICS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Basic descriptive statistics\n",
    "    basic_stats = df[numeric_vars].describe()\n",
    "    \n",
    "    # Additional statistics\n",
    "    additional_stats = pd.DataFrame(index=numeric_vars)\n",
    "    \n",
    "    for var in numeric_vars:\n",
    "        data = df[var].dropna()\n",
    "        if len(data) > 0:\n",
    "            # Central tendency\n",
    "            additional_stats.loc[var, 'median'] = data.median()\n",
    "            additional_stats.loc[var, 'mode'] = data.mode().iloc[0] if len(data.mode()) > 0 else np.nan\n",
    "            \n",
    "            # Dispersion\n",
    "            additional_stats.loc[var, 'variance'] = data.var()\n",
    "            additional_stats.loc[var, 'cv'] = (data.std() / data.mean()) * 100  # Coefficient of variation\n",
    "            additional_stats.loc[var, 'range'] = data.max() - data.min()\n",
    "            additional_stats.loc[var, 'iqr'] = data.quantile(0.75) - data.quantile(0.25)\n",
    "            \n",
    "            # Distribution shape\n",
    "            additional_stats.loc[var, 'skewness'] = stats.skew(data)\n",
    "            additional_stats.loc[var, 'kurtosis'] = stats.kurtosis(data)\n",
    "            \n",
    "            # Outlier detection (using IQR method)\n",
    "            Q1 = data.quantile(0.25)\n",
    "            Q3 = data.quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            outliers = ((data < lower_bound) | (data > upper_bound)).sum()\n",
    "            additional_stats.loc[var, 'outliers_count'] = outliers\n",
    "            additional_stats.loc[var, 'outliers_percent'] = (outliers / len(data)) * 100\n",
    "    \n",
    "    # Combine basic and additional statistics\n",
    "    comprehensive_stats = pd.concat([basic_stats.T, additional_stats], axis=1)\n",
    "    \n",
    "    # Display comprehensive statistics\n",
    "    print(comprehensive_stats.round(4))\n",
    "    \n",
    "    # Statistical interpretation\n",
    "    print(\"\\n📊 Statistical Interpretation:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    for var in numeric_vars[:5]:  # Show interpretation for first 5 variables\n",
    "        if var in comprehensive_stats.index:\n",
    "            stats_row = comprehensive_stats.loc[var]\n",
    "            print(f\"\\n🔍 {var}:\")\n",
    "            \n",
    "            # Central tendency interpretation\n",
    "            mean_val = stats_row['mean']\n",
    "            median_val = stats_row['median']\n",
    "            if abs(mean_val - median_val) / median_val < 0.1:\n",
    "                print(f\"   • Distribution: Approximately symmetric (mean ≈ median)\")\n",
    "            elif mean_val > median_val:\n",
    "                print(f\"   • Distribution: Right-skewed (mean > median)\")\n",
    "            else:\n",
    "                print(f\"   • Distribution: Left-skewed (mean < median)\")\n",
    "            \n",
    "            # Variability interpretation\n",
    "            cv = stats_row['cv']\n",
    "            if cv < 15:\n",
    "                print(f\"   • Variability: Low (CV = {cv:.1f}%)\")\n",
    "            elif cv < 35:\n",
    "                print(f\"   • Variability: Moderate (CV = {cv:.1f}%)\")\n",
    "            else:\n",
    "                print(f\"   • Variability: High (CV = {cv:.1f}%)\")\n",
    "            \n",
    "            # Outlier interpretation\n",
    "            outlier_pct = stats_row['outliers_percent']\n",
    "            if outlier_pct == 0:\n",
    "                print(f\"   • Outliers: None detected\")\n",
    "            elif outlier_pct < 5:\n",
    "                print(f\"   • Outliers: Few detected ({outlier_pct:.1f}%)\")\n",
    "            else:\n",
    "                print(f\"   • Outliers: Many detected ({outlier_pct:.1f}%) - investigate further\")\n",
    "\n",
    "# Categorical variables analysis\n",
    "if categorical_vars:\n",
    "    print(\"\\n\\n🏷️ CATEGORICAL VARIABLES - FREQUENCY ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    categorical_summary = []\n",
    "    \n",
    "    for var in categorical_vars:\n",
    "        value_counts = df[var].value_counts(dropna=False)\n",
    "        \n",
    "        cat_info = {\n",
    "            'Variable': var,\n",
    "            'Categories': df[var].nunique(),\n",
    "            'Missing': df[var].isnull().sum(),\n",
    "            'Missing_Pct': f\"{df[var].isnull().sum() / len(df) * 100:.1f}%\",\n",
    "            'Most_Frequent': value_counts.index[0] if len(value_counts) > 0 else 'N/A',\n",
    "            'Most_Freq_Count': value_counts.iloc[0] if len(value_counts) > 0 else 0,\n",
    "            'Most_Freq_Pct': f\"{value_counts.iloc[0] / df[var].count() * 100:.1f}%\" if len(value_counts) > 0 else '0%'\n",
    "        }\n",
    "        categorical_summary.append(cat_info)\n",
    "        \n",
    "        # Display frequency table for first few variables\n",
    "        if len(categorical_summary) <= 3:  # Show details for first 3 categorical variables\n",
    "            print(f\"\\n📊 {var} - Frequency Distribution:\")\n",
    "            print(\"-\" * 40)\n",
    "            freq_table = pd.DataFrame({\n",
    "                'Category': value_counts.index,\n",
    "                'Frequency': value_counts.values,\n",
    "                'Percentage': (value_counts.values / df[var].count() * 100).round(2)\n",
    "            })\n",
    "            print(freq_table.head(10).to_string(index=False))  # Show top 10 categories\n",
    "            if len(value_counts) > 10:\n",
    "                print(f\"... and {len(value_counts) - 10} more categories\")\n",
    "    \n",
    "    # Summary table for all categorical variables\n",
    "    print(f\"\\n📋 Categorical Variables Summary:\")\n",
    "    print(\"-\" * 40)\n",
    "    cat_summary_df = pd.DataFrame(categorical_summary)\n",
    "    print(cat_summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddbed3f",
   "metadata": {},
   "source": [
    "## 🔗 Comprehensive Correlation Analysis\n",
    "\n",
    "Multi-faceted correlation analysis including Pearson, Spearman, and visualization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8537e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive correlation analysis\n",
    "print(\"🔗 COMPREHENSIVE CORRELATION ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if len(numeric_vars) < 2:\n",
    "    print(\"⚠️ Warning: Less than 2 numeric variables found. Correlation analysis requires at least 2 numeric variables.\")\n",
    "else:\n",
    "    # Calculate correlation matrices\n",
    "    pearson_corr = df[numeric_vars].corr(method='pearson')\n",
    "    spearman_corr = df[numeric_vars].corr(method='spearman')\n",
    "    \n",
    "    print(f\"🔢 Analyzing correlations between {len(numeric_vars)} numeric variables\")\n",
    "    print(f\"📊 Total correlation pairs: {len(numeric_vars) * (len(numeric_vars) - 1) // 2}\")\n",
    "    \n",
    "    # Pearson correlation analysis\n",
    "    print(\"\\n📈 PEARSON CORRELATION MATRIX (Linear Relationships)\")\n",
    "    print(\"=\" * 55)\n",
    "    print(pearson_corr.round(3))\n",
    "    \n",
    "    # Find strong correlations\n",
    "    print(\"\\n🔍 STRONG CORRELATIONS (|r| > 0.7):\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    strong_correlations = []\n",
    "    for i in range(len(numeric_vars)):\n",
    "        for j in range(i+1, len(numeric_vars)):\n",
    "            var1, var2 = numeric_vars[i], numeric_vars[j]\n",
    "            r_value = pearson_corr.loc[var1, var2]\n",
    "            if abs(r_value) > 0.7:\n",
    "                # Calculate p-value\n",
    "                data1 = df[var1].dropna()\n",
    "                data2 = df[var2].dropna()\n",
    "                common_idx = data1.index.intersection(data2.index)\n",
    "                if len(common_idx) > 2:\n",
    "                    _, p_value = pearsonr(df.loc[common_idx, var1], df.loc[common_idx, var2])\n",
    "                    \n",
    "                    correlation_strength = \"Very Strong\" if abs(r_value) > 0.9 else \"Strong\"\n",
    "                    correlation_direction = \"Positive\" if r_value > 0 else \"Negative\"\n",
    "                    significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"ns\"\n",
    "                    \n",
    "                    strong_correlations.append({\n",
    "                        'Variable_1': var1,\n",
    "                        'Variable_2': var2,\n",
    "                        'Correlation': f\"{r_value:.3f}\",\n",
    "                        'Strength': correlation_strength,\n",
    "                        'Direction': correlation_direction,\n",
    "                        'P_Value': f\"{p_value:.6f}\",\n",
    "                        'Significance': significance,\n",
    "                        'N': len(common_idx)\n",
    "                    })\n",
    "    \n",
    "    if strong_correlations:\n",
    "        strong_corr_df = pd.DataFrame(strong_correlations)\n",
    "        print(strong_corr_df.to_string(index=False))\n",
    "    else:\n",
    "        print(\"No strong correlations (|r| > 0.7) found.\")\n",
    "    \n",
    "    # Moderate correlations\n",
    "    print(\"\\n📊 MODERATE CORRELATIONS (0.3 < |r| ≤ 0.7):\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    moderate_correlations = []\n",
    "    for i in range(len(numeric_vars)):\n",
    "        for j in range(i+1, len(numeric_vars)):\n",
    "            var1, var2 = numeric_vars[i], numeric_vars[j]\n",
    "            r_value = pearson_corr.loc[var1, var2]\n",
    "            if 0.3 < abs(r_value) <= 0.7:\n",
    "                moderate_correlations.append({\n",
    "                    'Variable_1': var1,\n",
    "                    'Variable_2': var2,\n",
    "                    'Correlation': f\"{r_value:.3f}\",\n",
    "                    'Interpretation': 'Moderate Positive' if r_value > 0 else 'Moderate Negative'\n",
    "                })\n",
    "    \n",
    "    if moderate_correlations:\n",
    "        moderate_corr_df = pd.DataFrame(moderate_correlations)\n",
    "        print(moderate_corr_df.head(10).to_string(index=False))  # Show top 10\n",
    "        if len(moderate_correlations) > 10:\n",
    "            print(f\"... and {len(moderate_correlations) - 10} more moderate correlations\")\n",
    "    else:\n",
    "        print(\"No moderate correlations found.\")\n",
    "    \n",
    "    # Spearman correlation comparison\n",
    "    print(\"\\n📊 SPEARMAN vs PEARSON CORRELATION COMPARISON\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"(Shows differences in linear vs monotonic relationships)\")\n",
    "    print()\n",
    "    \n",
    "    correlation_comparison = []\n",
    "    for i in range(len(numeric_vars)):\n",
    "        for j in range(i+1, len(numeric_vars)):\n",
    "            var1, var2 = numeric_vars[i], numeric_vars[j]\n",
    "            pearson_r = pearson_corr.loc[var1, var2]\n",
    "            spearman_r = spearman_corr.loc[var1, var2]\n",
    "            difference = abs(spearman_r - pearson_r)\n",
    "            \n",
    "            if difference > 0.1:  # Significant difference suggests non-linear relationship\n",
    "                correlation_comparison.append({\n",
    "                    'Variable_1': var1,\n",
    "                    'Variable_2': var2,\n",
    "                    'Pearson_r': f\"{pearson_r:.3f}\",\n",
    "                    'Spearman_r': f\"{spearman_r:.3f}\",\n",
    "                    'Difference': f\"{difference:.3f}\",\n",
    "                    'Interpretation': 'Non-linear relationship likely' if difference > 0.2 else 'Some non-linearity'\n",
    "                })\n",
    "    \n",
    "    if correlation_comparison:\n",
    "        comparison_df = pd.DataFrame(correlation_comparison)\n",
    "        print(\"Relationships with notable Pearson vs Spearman differences:\")\n",
    "        print(comparison_df.to_string(index=False))\n",
    "    else:\n",
    "        print(\"No significant differences between Pearson and Spearman correlations detected.\")\n",
    "    \n",
    "    # Statistical significance summary\n",
    "    print(\"\\n📋 CORRELATION SIGNIFICANCE SUMMARY\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    significance_counts = {'***': 0, '**': 0, '*': 0, 'ns': 0}\n",
    "    total_tests = 0\n",
    "    \n",
    "    for i in range(len(numeric_vars)):\n",
    "        for j in range(i+1, len(numeric_vars)):\n",
    "            var1, var2 = numeric_vars[i], numeric_vars[j]\n",
    "            data1 = df[var1].dropna()\n",
    "            data2 = df[var2].dropna()\n",
    "            common_idx = data1.index.intersection(data2.index)\n",
    "            \n",
    "            if len(common_idx) > 2:\n",
    "                _, p_value = pearsonr(df.loc[common_idx, var1], df.loc[common_idx, var2])\n",
    "                total_tests += 1\n",
    "                \n",
    "                if p_value < 0.001:\n",
    "                    significance_counts['***'] += 1\n",
    "                elif p_value < 0.01:\n",
    "                    significance_counts['**'] += 1\n",
    "                elif p_value < 0.05:\n",
    "                    significance_counts['*'] += 1\n",
    "                else:\n",
    "                    significance_counts['ns'] += 1\n",
    "    \n",
    "    print(f\"Total correlation tests performed: {total_tests}\")\n",
    "    print(f\"Highly significant (p < 0.001): {significance_counts['***']} ({significance_counts['***']/total_tests*100:.1f}%)\")\n",
    "    print(f\"Very significant (p < 0.01): {significance_counts['**']} ({significance_counts['**']/total_tests*100:.1f}%)\")\n",
    "    print(f\"Significant (p < 0.05): {significance_counts['*']} ({significance_counts['*']/total_tests*100:.1f}%)\")\n",
    "    print(f\"Not significant (p ≥ 0.05): {significance_counts['ns']} ({significance_counts['ns']/total_tests*100:.1f}%)\")\n",
    "    \n",
    "    print(\"\\nLegend: *** p<0.001, ** p<0.01, * p<0.05, ns = not significant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9cefab",
   "metadata": {},
   "source": [
    "## 📊 Advanced Correlation Visualizations\n",
    "\n",
    "Interactive and static visualizations to explore correlation patterns and relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4787615b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced correlation visualizations\n",
    "print(\"📊 ADVANCED CORRELATION VISUALIZATIONS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "if len(numeric_vars) >= 2:\n",
    "    # 1. Interactive correlation heatmap\n",
    "    print(\"Creating interactive correlation heatmap...\")\n",
    "    \n",
    "    fig_heatmap = go.Figure(data=go.Heatmap(\n",
    "        z=pearson_corr.values,\n",
    "        x=pearson_corr.columns,\n",
    "        y=pearson_corr.columns,\n",
    "        colorscale='RdBu',\n",
    "        zmid=0,\n",
    "        text=pearson_corr.round(3).values,\n",
    "        texttemplate=\"%{text}\",\n",
    "        textfont={\"size\": 10},\n",
    "        hovertemplate=\"Variable 1: %{y}<br>Variable 2: %{x}<br>Correlation: %{z:.3f}<extra></extra>\"\n",
    "    ))\n",
    "    \n",
    "    fig_heatmap.update_layout(\n",
    "        title=\"🔗 Interactive Pearson Correlation Matrix\",\n",
    "        width=800,\n",
    "        height=700,\n",
    "        font=dict(size=12)\n",
    "    )\n",
    "    \n",
    "    fig_heatmap.show()\n",
    "    \n",
    "    # 2. Scatter plot matrix for key variables (top correlated pairs)\n",
    "    if len(strong_correlations) > 0:\n",
    "        print(\"Creating scatter plots for strongly correlated variable pairs...\")\n",
    "        \n",
    "        # Select top 4 strong correlations for visualization\n",
    "        top_pairs = strong_correlations[:4] if len(strong_correlations) >= 4 else strong_correlations\n",
    "        \n",
    "        fig_scatter = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=[f\"{pair['Variable_1']} vs {pair['Variable_2']} (r={pair['Correlation']})\" \n",
    "                          for pair in top_pairs],\n",
    "            vertical_spacing=0.15,\n",
    "            horizontal_spacing=0.1\n",
    "        )\n",
    "        \n",
    "        for i, pair in enumerate(top_pairs):\n",
    "            row = (i // 2) + 1\n",
    "            col = (i % 2) + 1\n",
    "            \n",
    "            var1, var2 = pair['Variable_1'], pair['Variable_2']\n",
    "            \n",
    "            # Get clean data for both variables\n",
    "            clean_data = df[[var1, var2]].dropna()\n",
    "            \n",
    "            fig_scatter.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=clean_data[var1],\n",
    "                    y=clean_data[var2],\n",
    "                    mode='markers',\n",
    "                    marker=dict(size=6, opacity=0.6, color=enterprise_colors['primary']),\n",
    "                    name=f\"{var1} vs {var2}\",\n",
    "                    showlegend=False,\n",
    "                    hovertemplate=f\"{var1}: %{{x:.2f}}<br>{var2}: %{{y:.2f}}<extra></extra>\"\n",
    "                ),\n",
    "                row=row, col=col\n",
    "            )\n",
    "            \n",
    "            # Add trend line\n",
    "            z = np.polyfit(clean_data[var1], clean_data[var2], 1)\n",
    "            p = np.poly1d(z)\n",
    "            x_trend = np.linspace(clean_data[var1].min(), clean_data[var1].max(), 100)\n",
    "            \n",
    "            fig_scatter.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=x_trend,\n",
    "                    y=p(x_trend),\n",
    "                    mode='lines',\n",
    "                    line=dict(color=enterprise_colors['secondary'], width=2),\n",
    "                    name='Trend',\n",
    "                    showlegend=False,\n",
    "                    hoverinfo='skip'\n",
    "                ),\n",
    "                row=row, col=col\n",
    "            )\n",
    "        \n",
    "        fig_scatter.update_layout(\n",
    "            title=\"📈 Scatter Plots for Strongest Correlations\",\n",
    "            height=700,\n",
    "            width=1000,\n",
    "            font=dict(size=12)\n",
    "        )\n",
    "        \n",
    "        fig_scatter.show()\n",
    "    \n",
    "    # 3. Hierarchical clustering of correlations\n",
    "    if len(numeric_vars) >= 3:\n",
    "        print(\"Creating correlation dendrogram for variable clustering...\")\n",
    "        \n",
    "        from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "        from scipy.spatial.distance import squareform\n",
    "        \n",
    "        # Convert correlation to distance\n",
    "        distance_matrix = 1 - abs(pearson_corr)\n",
    "        \n",
    "        # Perform hierarchical clustering\n",
    "        condensed_distances = squareform(distance_matrix)\n",
    "        linkage_matrix = linkage(condensed_distances, method='average')\n",
    "        \n",
    "        # Create dendrogram\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        dendrogram(linkage_matrix, labels=numeric_vars, orientation='top', leaf_rotation=45)\n",
    "        plt.title('🌳 Variable Clustering Based on Correlations', fontsize=16, pad=20)\n",
    "        plt.xlabel('Variables', fontsize=12)\n",
    "        plt.ylabel('Distance (1 - |Correlation|)', fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # 4. Correlation network visualization\n",
    "    if len(strong_correlations) > 0:\n",
    "        print(\"Creating correlation network for strong relationships...\")\n",
    "        \n",
    "        import networkx as nx\n",
    "        \n",
    "        # Create network graph\n",
    "        G = nx.Graph()\n",
    "        \n",
    "        # Add nodes (variables)\n",
    "        for var in numeric_vars:\n",
    "            G.add_node(var)\n",
    "        \n",
    "        # Add edges (strong correlations)\n",
    "        for corr in strong_correlations:\n",
    "            weight = abs(float(corr['Correlation']))\n",
    "            G.add_edge(corr['Variable_1'], corr['Variable_2'], weight=weight)\n",
    "        \n",
    "        # Calculate layout\n",
    "        pos = nx.spring_layout(G, k=1, iterations=50)\n",
    "        \n",
    "        # Create edge traces\n",
    "        edge_x = []\n",
    "        edge_y = []\n",
    "        edge_info = []\n",
    "        \n",
    "        for edge in G.edges():\n",
    "            x0, y0 = pos[edge[0]]\n",
    "            x1, y1 = pos[edge[1]]\n",
    "            edge_x.extend([x0, x1, None])\n",
    "            edge_y.extend([y0, y1, None])\n",
    "            \n",
    "            weight = G[edge[0]][edge[1]]['weight']\n",
    "            edge_info.append(f\"{edge[0]} - {edge[1]}: r = {weight:.3f}\")\n",
    "        \n",
    "        edge_trace = go.Scatter(x=edge_x, y=edge_y,\n",
    "                               line=dict(width=2, color='#888'),\n",
    "                               hoverinfo='none',\n",
    "                               mode='lines')\n",
    "        \n",
    "        # Create node traces\n",
    "        node_x = []\n",
    "        node_y = []\n",
    "        node_text = []\n",
    "        node_info = []\n",
    "        \n",
    "        for node in G.nodes():\n",
    "            x, y = pos[node]\n",
    "            node_x.append(x)\n",
    "            node_y.append(y)\n",
    "            node_text.append(node)\n",
    "            \n",
    "            # Count connections\n",
    "            adjacencies = list(G.neighbors(node))\n",
    "            node_info.append(f'{node}<br>Connections: {len(adjacencies)}')\n",
    "        \n",
    "        node_trace = go.Scatter(x=node_x, y=node_y,\n",
    "                               mode='markers+text',\n",
    "                               hoverinfo='text',\n",
    "                               text=node_text,\n",
    "                               textposition=\"middle center\",\n",
    "                               hovertext=node_info,\n",
    "                               marker=dict(showscale=True,\n",
    "                                         colorscale='Viridis',\n",
    "                                         size=20,\n",
    "                                         colorbar=dict(thickness=15,\n",
    "                                                     xanchor=\"left\",\n",
    "                                                     titleside=\"right\")))\n",
    "        \n",
    "        # Color nodes by number of connections\n",
    "        node_adjacencies = []\n",
    "        for node in G.nodes():\n",
    "            node_adjacencies.append(len(list(G.neighbors(node))))\n",
    "        \n",
    "        node_trace.marker.color = node_adjacencies\n",
    "        \n",
    "        # Create the figure\n",
    "        fig_network = go.Figure(data=[edge_trace, node_trace],\n",
    "                               layout=go.Layout(\n",
    "                                   title='🕸️ Correlation Network (Strong Relationships Only)',\n",
    "                                   titlefont_size=16,\n",
    "                                   showlegend=False,\n",
    "                                   hovermode='closest',\n",
    "                                   margin=dict(b=20,l=5,r=5,t=40),\n",
    "                                   annotations=[ dict(\n",
    "                                       text=\"Variables connected by strong correlations (|r| > 0.7)\",\n",
    "                                       showarrow=False,\n",
    "                                       xref=\"paper\", yref=\"paper\",\n",
    "                                       x=0.005, y=-0.002 ) ],\n",
    "                                   xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                                   yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                                   width=800,\n",
    "                                   height=600))\n",
    "        \n",
    "        fig_network.show()\n",
    "\n",
    "else:\n",
    "    print(\"⚠️ Insufficient numeric variables for correlation visualization.\")\n",
    "\n",
    "print(\"\\n✅ Correlation analysis and visualizations complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8486fb60",
   "metadata": {},
   "source": [
    "## 📋 Data Quality Assessment & Summary\n",
    "\n",
    "Comprehensive evaluation of data integrity, missing patterns, and analysis summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea86183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive data quality assessment\n",
    "print(\"📋 COMPREHENSIVE DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Missing data analysis\n",
    "print(\"🔍 Missing Data Analysis:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "missing_summary = df.isnull().sum().sort_values(ascending=False)\n",
    "missing_percentage = (missing_summary / len(df) * 100).round(2)\n",
    "\n",
    "missing_analysis = pd.DataFrame({\n",
    "    'Variable': missing_summary.index,\n",
    "    'Missing_Count': missing_summary.values,\n",
    "    'Missing_Percentage': missing_percentage.values,\n",
    "    'Complete_Count': len(df) - missing_summary.values,\n",
    "    'Data_Quality': ['Excellent' if x < 5 else 'Good' if x < 15 else 'Fair' if x < 30 else 'Poor' \n",
    "                    for x in missing_percentage.values]\n",
    "})\n",
    "\n",
    "print(missing_analysis[missing_analysis['Missing_Count'] > 0].to_string(index=False))\n",
    "\n",
    "if missing_analysis['Missing_Count'].sum() == 0:\n",
    "    print(\"🎉 Excellent! No missing values detected in the dataset.\")\n",
    "else:\n",
    "    variables_with_missing = (missing_analysis['Missing_Count'] > 0).sum()\n",
    "    print(f\"\\n📊 Missing Data Summary:\")\n",
    "    print(f\"   • Variables with missing data: {variables_with_missing} of {len(df.columns)}\")\n",
    "    print(f\"   • Total missing values: {missing_analysis['Missing_Count'].sum():,}\")\n",
    "    print(f\"   • Overall completeness: {(1 - missing_analysis['Missing_Count'].sum() / (len(df) * len(df.columns))) * 100:.1f}%\")\n",
    "\n",
    "# Data type consistency\n",
    "print(f\"\\n🔤 Data Type Analysis:\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "type_analysis = df.dtypes.value_counts()\n",
    "print(f\"Data type distribution:\")\n",
    "for dtype, count in type_analysis.items():\n",
    "    print(f\"   • {dtype}: {count} variables ({count/len(df.columns)*100:.1f}%)\")\n",
    "\n",
    "# Duplicate analysis\n",
    "print(f\"\\n🔄 Duplicate Records Analysis:\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "duplicate_count = df.duplicated().sum()\n",
    "if duplicate_count > 0:\n",
    "    print(f\"⚠️ Found {duplicate_count} duplicate records ({duplicate_count/len(df)*100:.2f}%)\")\n",
    "    print(f\"   • Unique records: {len(df) - duplicate_count:,}\")\n",
    "    print(f\"   • Total records: {len(df):,}\")\n",
    "else:\n",
    "    print(\"✅ No duplicate records found.\")\n",
    "\n",
    "# Variable uniqueness analysis\n",
    "print(f\"\\n🎯 Variable Uniqueness Analysis:\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "uniqueness_analysis = []\n",
    "for col in df.columns:\n",
    "    unique_count = df[col].nunique()\n",
    "    unique_percentage = (unique_count / len(df)) * 100\n",
    "    \n",
    "    if unique_percentage == 100:\n",
    "        uniqueness_type = \"Identifier (100% unique)\"\n",
    "    elif unique_percentage > 95:\n",
    "        uniqueness_type = \"Near-identifier (>95% unique)\"\n",
    "    elif unique_percentage > 50:\n",
    "        uniqueness_type = \"High variability\"\n",
    "    elif unique_percentage > 10:\n",
    "        uniqueness_type = \"Moderate variability\"\n",
    "    else:\n",
    "        uniqueness_type = \"Low variability\"\n",
    "    \n",
    "    uniqueness_analysis.append({\n",
    "        'Variable': col,\n",
    "        'Unique_Values': unique_count,\n",
    "        'Unique_Percentage': f\"{unique_percentage:.1f}%\",\n",
    "        'Classification': uniqueness_type\n",
    "    })\n",
    "\n",
    "uniqueness_df = pd.DataFrame(uniqueness_analysis)\n",
    "print(uniqueness_df.to_string(index=False))\n",
    "\n",
    "# Overall data quality score\n",
    "print(f\"\\n🏆 OVERALL DATA QUALITY SCORE\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Calculate quality score (0-100)\n",
    "completeness_score = (1 - missing_analysis['Missing_Count'].sum() / (len(df) * len(df.columns))) * 100\n",
    "uniqueness_score = 100 if duplicate_count == 0 else max(0, 100 - (duplicate_count / len(df) * 100))\n",
    "consistency_score = 90  # Base score, could be enhanced with more sophisticated checks\n",
    "\n",
    "overall_score = (completeness_score * 0.4 + uniqueness_score * 0.3 + consistency_score * 0.3)\n",
    "\n",
    "print(f\"📊 Data Quality Components:\")\n",
    "print(f\"   • Completeness: {completeness_score:.1f}/100 (40% weight)\")\n",
    "print(f\"   • Uniqueness: {uniqueness_score:.1f}/100 (30% weight)\")\n",
    "print(f\"   • Consistency: {consistency_score:.1f}/100 (30% weight)\")\n",
    "print(f\"\\n🎯 Overall Data Quality Score: {overall_score:.1f}/100\")\n",
    "\n",
    "if overall_score >= 90:\n",
    "    quality_grade = \"Excellent (A)\"\n",
    "elif overall_score >= 80:\n",
    "    quality_grade = \"Good (B)\"\n",
    "elif overall_score >= 70:\n",
    "    quality_grade = \"Fair (C)\"\n",
    "else:\n",
    "    quality_grade = \"Needs Improvement (D)\"\n",
    "\n",
    "print(f\"📈 Data Quality Grade: {quality_grade}\")\n",
    "\n",
    "# Analysis summary and recommendations\n",
    "print(f\"\\n📝 ANALYSIS SUMMARY & RECOMMENDATIONS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "print(f\"🔍 Dataset Characteristics:\")\n",
    "print(f\"   • Sample size: {len(df):,} observations\")\n",
    "print(f\"   • Variable count: {len(df.columns)} features\")\n",
    "print(f\"   • Numeric variables: {len(numeric_vars)}\")\n",
    "print(f\"   • Categorical variables: {len(categorical_vars)}\")\n",
    "print(f\"   • Data quality: {quality_grade}\")\n",
    "\n",
    "if len(numeric_vars) >= 2:\n",
    "    strong_corr_count = len(strong_correlations) if 'strong_correlations' in locals() else 0\n",
    "    moderate_corr_count = len(moderate_correlations) if 'moderate_correlations' in locals() else 0\n",
    "    \n",
    "    print(f\"\\n🔗 Correlation Insights:\")\n",
    "    print(f\"   • Strong correlations (|r| > 0.7): {strong_corr_count}\")\n",
    "    print(f\"   • Moderate correlations (0.3-0.7): {moderate_corr_count}\")\n",
    "    \n",
    "    if strong_corr_count > 0:\n",
    "        print(f\"   • Consider multicollinearity in modeling\")\n",
    "        print(f\"   • Potential for dimensionality reduction\")\n",
    "\n",
    "print(f\"\\n💡 Recommendations for Further Analysis:\")\n",
    "recommendations = []\n",
    "\n",
    "if missing_analysis['Missing_Count'].sum() > 0:\n",
    "    recommendations.append(\"Handle missing values through imputation or deletion\")\n",
    "\n",
    "if duplicate_count > 0:\n",
    "    recommendations.append(\"Investigate and remove duplicate records\")\n",
    "\n",
    "if len(numeric_vars) >= 3:\n",
    "    recommendations.append(\"Consider principal component analysis (PCA) for dimensionality reduction\")\n",
    "\n",
    "if strong_corr_count > 2:\n",
    "    recommendations.append(\"Examine multicollinearity before regression modeling\")\n",
    "\n",
    "if len(categorical_vars) > 0:\n",
    "    recommendations.append(\"Perform chi-square tests for categorical associations\")\n",
    "\n",
    "recommendations.extend([\n",
    "    \"Validate findings with domain expertise\",\n",
    "    \"Consider advanced statistical modeling based on research questions\",\n",
    "    \"Explore temporal patterns if time variables are present\"\n",
    "])\n",
    "\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"   {i}. {rec}\")\n",
    "\n",
    "print(f\"\\n🎯 Analysis Complete! The dataset has been thoroughly examined.\")\n",
    "print(f\"📊 Ready for advanced statistical modeling and hypothesis testing.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
