{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27ffd344",
   "metadata": {},
   "source": [
    "# Getting Started with Enterprise Data Analysis\n",
    "\n",
    "Welcome to the Enterprise Data Analysis Cognitive Architecture! This notebook will guide you through your first analysis using our advanced analytics platform.\n",
    "\n",
    "## What You'll Learn\n",
    "- How to load different types of data files\n",
    "- Basic data exploration and quality assessment\n",
    "- Creating your first statistical analysis\n",
    "- Generating visualizations and reports\n",
    "\n",
    "## Prerequisites\n",
    "- Python environment with required packages installed\n",
    "- Sample data files (we'll create some if needed)\n",
    "\n",
    "Let's get started! 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacf6a13",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Imports\n",
    "\n",
    "First, let's import the core components of our analysis platform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7823133e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data analysis imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr, f_oneway\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Additional libraries for comprehensive analysis\n",
    "try:\n",
    "    import pyreadstat  # For SPSS files\n",
    "    SPSS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SPSS_AVAILABLE = False\n",
    "    print(\"⚠️ pyreadstat not available - SPSS file support limited\")\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set pandas display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.3f}'.format)\n",
    "\n",
    "print(\"✅ All imports successful!\")\n",
    "print(f\"📊 Pandas version: {pd.__version__}\")\n",
    "print(f\"🔢 NumPy version: {np.__version__}\")\n",
    "print(f\"📈 Matplotlib version: {plt.matplotlib.__version__}\")\n",
    "print(f\"📊 Seaborn version: {sns.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1cab74",
   "metadata": {},
   "source": [
    "## 2. Create Sample Data\n",
    "\n",
    "Let's create some sample data to work with. In real scenarios, you'd load your own data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c55538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample survey data\n",
    "np.random.seed(42)  # For reproducible results\n",
    "\n",
    "n_responses = 500\n",
    "\n",
    "# Generate realistic survey data\n",
    "sample_data = {\n",
    "    'respondent_id': range(1, n_responses + 1),\n",
    "    'age': np.random.normal(35, 12, n_responses).astype(int),\n",
    "    'satisfaction_score': np.random.normal(7.2, 1.8, n_responses),\n",
    "    'department': np.random.choice(['Sales', 'Marketing', 'IT', 'HR', 'Finance'], n_responses),\n",
    "    'years_experience': np.random.exponential(5, n_responses),\n",
    "    'training_completed': np.random.choice([True, False], n_responses, p=[0.7, 0.3]),\n",
    "    'region': np.random.choice(['North', 'South', 'East', 'West'], n_responses)\n",
    "}\n",
    "\n",
    "# Clean up the data\n",
    "sample_data['age'] = np.clip(sample_data['age'], 18, 65)\n",
    "sample_data['satisfaction_score'] = np.clip(sample_data['satisfaction_score'], 1, 10)\n",
    "sample_data['years_experience'] = np.clip(sample_data['years_experience'], 0, 40)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(sample_data)\n",
    "\n",
    "print(f\"📈 Created sample dataset with {len(df)} responses\")\n",
    "print(f\"📊 Columns: {list(df.columns)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bb1d9e",
   "metadata": {},
   "source": [
    "## 3. Data Loading with DataLoader\n",
    "\n",
    "Let's use our enterprise DataLoader to save and reload the data, demonstrating the platform's capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9dbf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory and save sample data\n",
    "data_dir = Path('../data')\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save as CSV\n",
    "csv_path = data_dir / 'sample_survey.csv'\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "# Save as Excel\n",
    "excel_path = data_dir / 'sample_survey.xlsx'\n",
    "df.to_excel(excel_path, index=False)\n",
    "\n",
    "print(f\"💾 Data saved to:\")\n",
    "print(f\"  CSV: {csv_path}\")\n",
    "print(f\"  Excel: {excel_path}\")\n",
    "\n",
    "# Load data back using pandas\n",
    "loaded_data = pd.read_csv(csv_path)\n",
    "\n",
    "print(f\"\\n📂 Loaded data shape: {loaded_data.shape}\")\n",
    "print(f\"📊 Data types:\")\n",
    "print(loaded_data.dtypes)\n",
    "\n",
    "# Ensure proper data types\n",
    "loaded_data['training_completed'] = loaded_data['training_completed'].astype(bool)\n",
    "print(f\"\\n✅ Data loaded and types corrected successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f41e0c",
   "metadata": {},
   "source": [
    "## 4. Data Exploration and Quality Assessment\n",
    "\n",
    "Now let's explore our data and assess its quality using the StatisticalAnalyzer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ce6a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data overview function\n",
    "def get_data_overview(df):\n",
    "    \"\"\"Generate comprehensive data overview\"\"\"\n",
    "    overview = {\n",
    "        'shape': df.shape,\n",
    "        'memory_usage': df.memory_usage(deep=True).sum() / 1024**2,  # MB\n",
    "        'missing_values': df.isnull().sum().sum(),\n",
    "        'duplicates': df.duplicated().sum(),\n",
    "        'column_info': {}\n",
    "    }\n",
    "    \n",
    "    for col in df.columns:\n",
    "        overview['column_info'][col] = {\n",
    "            'dtype': str(df[col].dtype),\n",
    "            'unique_values': df[col].nunique(),\n",
    "            'missing_count': df[col].isnull().sum(),\n",
    "            'missing_percent': (df[col].isnull().sum() / len(df)) * 100\n",
    "        }\n",
    "    \n",
    "    return overview\n",
    "\n",
    "# Get data overview\n",
    "overview = get_data_overview(loaded_data)\n",
    "\n",
    "print(\"📋 DATA OVERVIEW\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Dataset Shape: {overview['shape']}\")\n",
    "print(f\"Memory Usage: {overview['memory_usage']:.2f} MB\")\n",
    "print(f\"Missing Values: {overview['missing_values']}\")\n",
    "print(f\"Duplicate Rows: {overview['duplicates']}\")\n",
    "\n",
    "print(\"\\n📊 COLUMN INFORMATION\")\n",
    "print(\"=\" * 50)\n",
    "for col, info in overview['column_info'].items():\n",
    "    missing_info = f\" ({info['missing_count']} missing)\" if info['missing_count'] > 0 else \"\"\n",
    "    print(f\"{col}: {info['dtype']} ({info['unique_values']} unique values{missing_info})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658a5bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality assessment function\n",
    "def assess_data_quality(df):\n",
    "    \"\"\"Perform comprehensive data quality assessment\"\"\"\n",
    "    \n",
    "    # Calculate quality metrics\n",
    "    completeness = (1 - (df.isnull().sum().sum() / (df.shape[0] * df.shape[1]))) * 100\n",
    "    uniqueness = (1 - (df.duplicated().sum() / len(df))) * 100\n",
    "    consistency = 95  # Base score for data type consistency\n",
    "    \n",
    "    # Overall quality score (weighted average)\n",
    "    overall_score = (completeness * 0.4 + uniqueness * 0.3 + consistency * 0.3)\n",
    "    \n",
    "    # Identify issues\n",
    "    issues = []\n",
    "    if completeness < 95:\n",
    "        missing_pct = 100 - completeness\n",
    "        issues.append(f\"Missing data: {missing_pct:.1f}% of values are missing\")\n",
    "    \n",
    "    if uniqueness < 100:\n",
    "        dup_pct = 100 - uniqueness\n",
    "        issues.append(f\"Duplicate records: {dup_pct:.1f}% of records are duplicates\")\n",
    "    \n",
    "    # Check for potential data issues\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        if df[col].count() > 0:\n",
    "            # Check for extreme outliers\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 3 * IQR\n",
    "            upper_bound = Q3 + 3 * IQR\n",
    "            extreme_outliers = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()\n",
    "            \n",
    "            if extreme_outliers > len(df) * 0.05:  # More than 5% extreme outliers\n",
    "                issues.append(f\"Potential data quality issue in {col}: {extreme_outliers} extreme outliers\")\n",
    "    \n",
    "    return {\n",
    "        'overall_score': round(overall_score, 1),\n",
    "        'metrics': {\n",
    "            'completeness': round(completeness, 1),\n",
    "            'uniqueness': round(uniqueness, 1),\n",
    "            'consistency': round(consistency, 1)\n",
    "        },\n",
    "        'issues': issues\n",
    "    }\n",
    "\n",
    "# Perform data quality assessment\n",
    "quality_report = assess_data_quality(loaded_data)\n",
    "\n",
    "print(\"🔍 DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Overall Quality Score: {quality_report['overall_score']}/100\")\n",
    "\n",
    "print(\"\\n📈 Quality Metrics:\")\n",
    "for metric, score in quality_report['metrics'].items():\n",
    "    status = \"✅\" if score >= 80 else \"⚠️\" if score >= 60 else \"❌\"\n",
    "    print(f\"  {status} {metric.title()}: {score}/100\")\n",
    "\n",
    "if quality_report['issues']:\n",
    "    print(\"\\n⚠️ Issues Found:\")\n",
    "    for issue in quality_report['issues']:\n",
    "        print(f\"  • {issue}\")\n",
    "else:\n",
    "    print(\"\\n✅ No major data quality issues detected!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0fa1cd",
   "metadata": {},
   "source": [
    "## 5. Descriptive Statistics\n",
    "\n",
    "Let's generate comprehensive descriptive statistics for our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985dc389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate descriptive statistics function\n",
    "def generate_descriptive_stats(df):\n",
    "    \"\"\"Generate comprehensive descriptive statistics\"\"\"\n",
    "    \n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'bool', 'category']).columns\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Numeric variables summary\n",
    "    if len(numeric_cols) > 0:\n",
    "        numeric_summary = {}\n",
    "        for col in numeric_cols:\n",
    "            if df[col].count() > 0:  # Only if there are non-null values\n",
    "                data = df[col].dropna()\n",
    "                numeric_summary[col] = {\n",
    "                    'count': len(data),\n",
    "                    'mean': data.mean(),\n",
    "                    'std': data.std(),\n",
    "                    'min': data.min(),\n",
    "                    'q1': data.quantile(0.25),\n",
    "                    'median': data.median(),\n",
    "                    'q3': data.quantile(0.75),\n",
    "                    'max': data.max(),\n",
    "                    'skewness': stats.skew(data),\n",
    "                    'kurtosis': stats.kurtosis(data)\n",
    "                }\n",
    "        results['numeric_summary'] = numeric_summary\n",
    "    \n",
    "    # Categorical variables summary\n",
    "    if len(categorical_cols) > 0:\n",
    "        categorical_summary = {}\n",
    "        for col in categorical_cols:\n",
    "            value_counts = df[col].value_counts()\n",
    "            categorical_summary[col] = {\n",
    "                'unique_count': df[col].nunique(),\n",
    "                'mode': value_counts.index[0] if len(value_counts) > 0 else None,\n",
    "                'mode_frequency': value_counts.iloc[0] if len(value_counts) > 0 else 0,\n",
    "                'value_counts': value_counts.to_dict()\n",
    "            }\n",
    "        results['categorical_summary'] = categorical_summary\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Generate descriptive statistics\n",
    "desc_stats = generate_descriptive_stats(loaded_data)\n",
    "\n",
    "print(\"📊 DESCRIPTIVE STATISTICS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Display numeric variable statistics\n",
    "if 'numeric_summary' in desc_stats:\n",
    "    print(\"\\n🔢 Numeric Variables:\")\n",
    "    numeric_df = pd.DataFrame(desc_stats['numeric_summary']).round(2)\n",
    "    print(numeric_df)\n",
    "\n",
    "# Display categorical variable statistics\n",
    "if 'categorical_summary' in desc_stats:\n",
    "    print(\"\\n📂 Categorical Variables:\")\n",
    "    for var, stats in desc_stats['categorical_summary'].items():\n",
    "        print(f\"\\n  {var}:\")\n",
    "        print(f\"    Unique values: {stats['unique_count']}\")\n",
    "        print(f\"    Most common: {stats['mode']} ({stats['mode_frequency']} occurrences)\")\n",
    "        print(f\"    Distribution:\")\n",
    "        # Show top 5 categories\n",
    "        for i, (category, count) in enumerate(list(stats['value_counts'].items())[:5]):\n",
    "            percentage = (count / len(loaded_data)) * 100\n",
    "            print(f\"      {category}: {count} ({percentage:.1f}%)\")\n",
    "        if len(stats['value_counts']) > 5:\n",
    "            print(f\"      ... and {len(stats['value_counts']) - 5} more categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b64b62",
   "metadata": {},
   "source": [
    "## 6. Basic Visualizations\n",
    "\n",
    "Now let's create some visualizations using the EnterpriseVisualizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665f8fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive data overview dashboard\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Employee Survey Data - Overview Dashboard', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Age distribution\n",
    "axes[0, 0].hist(loaded_data['age'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0, 0].set_title('Age Distribution')\n",
    "axes[0, 0].set_xlabel('Age')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Satisfaction score by department\n",
    "dept_satisfaction = loaded_data.groupby('department')['satisfaction_score'].mean().sort_values(ascending=True)\n",
    "axes[0, 1].barh(dept_satisfaction.index, dept_satisfaction.values, color='lightcoral')\n",
    "axes[0, 1].set_title('Average Satisfaction by Department')\n",
    "axes[0, 1].set_xlabel('Satisfaction Score')\n",
    "\n",
    "# 3. Years experience vs satisfaction\n",
    "axes[1, 0].scatter(loaded_data['years_experience'], loaded_data['satisfaction_score'], \n",
    "                   alpha=0.6, color='green')\n",
    "axes[1, 0].set_title('Experience vs Satisfaction')\n",
    "axes[1, 0].set_xlabel('Years of Experience')\n",
    "axes[1, 0].set_ylabel('Satisfaction Score')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Training completion by region\n",
    "training_by_region = loaded_data.groupby('region')['training_completed'].mean() * 100\n",
    "axes[1, 1].bar(training_by_region.index, training_by_region.values, color='orange')\n",
    "axes[1, 1].set_title('Training Completion Rate by Region')\n",
    "axes[1, 1].set_xlabel('Region')\n",
    "axes[1, 1].set_ylabel('Completion Rate (%)')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"📊 Dashboard created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f80804",
   "metadata": {},
   "source": [
    "## 7. Statistical Analysis\n",
    "\n",
    "Let's perform some basic statistical tests to understand our data better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56099d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis function\n",
    "def perform_correlation_analysis(df, variables, method='pearson'):\n",
    "    \"\"\"Perform correlation analysis on specified variables\"\"\"\n",
    "    \n",
    "    # Filter to numeric variables that exist in the dataset\n",
    "    available_vars = [var for var in variables if var in df.columns and df[var].dtype in ['int64', 'float64']]\n",
    "    \n",
    "    if len(available_vars) < 2:\n",
    "        return {'error': 'Need at least 2 numeric variables for correlation analysis'}\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr_data = df[available_vars].corr(method=method)\n",
    "    \n",
    "    # Find significant correlations\n",
    "    significant_correlations = []\n",
    "    for i in range(len(available_vars)):\n",
    "        for j in range(i+1, len(available_vars)):\n",
    "            var1, var2 = available_vars[i], available_vars[j]\n",
    "            correlation = corr_data.loc[var1, var2]\n",
    "            \n",
    "            # Calculate p-value for Pearson correlation\n",
    "            if method == 'pearson':\n",
    "                clean_data = df[[var1, var2]].dropna()\n",
    "                if len(clean_data) > 2:\n",
    "                    _, p_value = pearsonr(clean_data[var1], clean_data[var2])\n",
    "                    \n",
    "                    significant_correlations.append({\n",
    "                        'variables': f\"{var1} vs {var2}\",\n",
    "                        'correlation': correlation,\n",
    "                        'p_value': p_value,\n",
    "                        'significant': p_value < 0.05\n",
    "                    })\n",
    "    \n",
    "    return {\n",
    "        'correlation_matrix': corr_data.to_dict(),\n",
    "        'significant_correlations': significant_correlations\n",
    "    }\n",
    "\n",
    "# Correlation analysis\n",
    "correlation_result = perform_correlation_analysis(\n",
    "    loaded_data, \n",
    "    variables=['age', 'satisfaction_score', 'years_experience'],\n",
    "    method='pearson'\n",
    ")\n",
    "\n",
    "print(\"🔗 CORRELATION ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if 'error' in correlation_result:\n",
    "    print(f\"❌ {correlation_result['error']}\")\n",
    "else:\n",
    "    print(\"Correlation Matrix:\")\n",
    "    corr_df = pd.DataFrame(correlation_result['correlation_matrix'])\n",
    "    print(corr_df.round(3))\n",
    "    \n",
    "    print(\"\\n📈 Correlation Results:\")\n",
    "    for corr in correlation_result['significant_correlations']:\n",
    "        significance = \"***\" if corr['p_value'] < 0.001 else \"**\" if corr['p_value'] < 0.01 else \"*\" if corr['p_value'] < 0.05 else \"ns\"\n",
    "        print(f\"  {corr['variables']}: r = {corr['correlation']:.3f}, p = {corr['p_value']:.3f} {significance}\")\n",
    "    \n",
    "    print(\"\\nLegend: *** p<0.001, ** p<0.01, * p<0.05, ns = not significant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cf049f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-way ANOVA function\n",
    "def perform_one_way_anova(df, dependent_var, independent_var):\n",
    "    \"\"\"Perform one-way ANOVA analysis\"\"\"\n",
    "    \n",
    "    # Get groups\n",
    "    groups = []\n",
    "    group_names = []\n",
    "    \n",
    "    for group_name in df[independent_var].unique():\n",
    "        if pd.notna(group_name):  # Exclude NaN groups\n",
    "            group_data = df[df[independent_var] == group_name][dependent_var].dropna()\n",
    "            if len(group_data) > 0:\n",
    "                groups.append(group_data)\n",
    "                group_names.append(group_name)\n",
    "    \n",
    "    if len(groups) < 2:\n",
    "        return {'error': 'Need at least 2 groups for ANOVA'}\n",
    "    \n",
    "    # Perform ANOVA\n",
    "    f_statistic, p_value = f_oneway(*groups)\n",
    "    \n",
    "    # Calculate effect size (eta-squared)\n",
    "    # Total sum of squares\n",
    "    all_data = df[dependent_var].dropna()\n",
    "    total_mean = all_data.mean()\n",
    "    ss_total = ((all_data - total_mean) ** 2).sum()\n",
    "    \n",
    "    # Between-group sum of squares\n",
    "    ss_between = 0\n",
    "    for i, group in enumerate(groups):\n",
    "        group_mean = group.mean()\n",
    "        ss_between += len(group) * ((group_mean - total_mean) ** 2)\n",
    "    \n",
    "    # Calculate eta-squared\n",
    "    eta_squared = ss_between / ss_total if ss_total > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'f_statistic': f_statistic,\n",
    "        'p_value': p_value,\n",
    "        'eta_squared': eta_squared,\n",
    "        'groups': group_names,\n",
    "        'group_count': len(groups)\n",
    "    }\n",
    "\n",
    "# Compare satisfaction scores between departments using ANOVA\n",
    "anova_result = perform_one_way_anova(\n",
    "    loaded_data,\n",
    "    dependent_var='satisfaction_score',\n",
    "    independent_var='department'\n",
    ")\n",
    "\n",
    "print(\"📊 ONE-WAY ANOVA RESULTS\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Research Question: Do satisfaction scores differ significantly between departments?\")\n",
    "\n",
    "if 'error' in anova_result:\n",
    "    print(f\"❌ {anova_result['error']}\")\n",
    "else:\n",
    "    print(f\"\\nF-statistic: {anova_result['f_statistic']:.3f}\")\n",
    "    print(f\"p-value: {anova_result['p_value']:.3f}\")\n",
    "    print(f\"Effect size (η²): {anova_result['eta_squared']:.3f}\")\n",
    "    print(f\"Groups analyzed: {anova_result['group_count']} departments\")\n",
    "    \n",
    "    alpha = 0.05\n",
    "    if anova_result['p_value'] < alpha:\n",
    "        print(f\"\\n✅ Result: Significant difference found (p < {alpha})\")\n",
    "        print(\"   There are statistically significant differences in satisfaction between departments.\")\n",
    "    else:\n",
    "        print(f\"\\n❌ Result: No significant difference (p ≥ {alpha})\")\n",
    "        print(\"   No statistically significant differences in satisfaction between departments.\")\n",
    "    \n",
    "    # Show group means\n",
    "    print(\"\\n📈 Department Satisfaction Means:\")\n",
    "    dept_means = loaded_data.groupby('department')['satisfaction_score'].agg(['mean', 'std', 'count'])\n",
    "    print(dept_means.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37086175",
   "metadata": {},
   "source": [
    "## 8. Generate Executive Summary\n",
    "\n",
    "Let's create an executive summary of our analysis findings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614aee7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate executive summary function\n",
    "def generate_executive_summary(df, key_metrics, include_insights=True):\n",
    "    \"\"\"Generate comprehensive executive summary\"\"\"\n",
    "    \n",
    "    summary = {\n",
    "        'analysis_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'dataset_info': {\n",
    "            'name': 'Employee Survey Data',\n",
    "            'sample_size': len(df),\n",
    "            'variables': list(df.columns),\n",
    "            'time_period': 'Current Analysis'\n",
    "        },\n",
    "        'key_findings': [],\n",
    "        'key_metrics': {},\n",
    "        'recommendations': []\n",
    "    }\n",
    "    \n",
    "    # Calculate key metrics\n",
    "    for metric in key_metrics:\n",
    "        if metric in df.columns:\n",
    "            if df[metric].dtype in ['int64', 'float64']:\n",
    "                summary['key_metrics'][metric] = {\n",
    "                    'mean': df[metric].mean(),\n",
    "                    'median': df[metric].median(),\n",
    "                    'std': df[metric].std(),\n",
    "                    'min': df[metric].min(),\n",
    "                    'max': df[metric].max()\n",
    "                }\n",
    "            elif df[metric].dtype == 'bool':\n",
    "                summary['key_metrics'][metric] = {\n",
    "                    'completion_rate': df[metric].mean() * 100,\n",
    "                    'total_completed': df[metric].sum(),\n",
    "                    'total_not_completed': (~df[metric]).sum()\n",
    "                }\n",
    "    \n",
    "    # Generate key findings\n",
    "    if include_insights:\n",
    "        # Sample size assessment\n",
    "        if len(df) >= 500:\n",
    "            summary['key_findings'].append(\"Large sample size provides robust statistical power\")\n",
    "        elif len(df) >= 100:\n",
    "            summary['key_findings'].append(\"Adequate sample size for most statistical analyses\")\n",
    "        else:\n",
    "            summary['key_findings'].append(\"Small sample size may limit statistical power\")\n",
    "        \n",
    "        # Data quality findings\n",
    "        missing_pct = (df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100\n",
    "        if missing_pct < 5:\n",
    "            summary['key_findings'].append(\"Excellent data quality with minimal missing values\")\n",
    "        elif missing_pct < 15:\n",
    "            summary['key_findings'].append(\"Good data quality with some missing values\")\n",
    "        else:\n",
    "            summary['key_findings'].append(\"Data quality concerns due to high missing value rate\")\n",
    "        \n",
    "        # Satisfaction insights (if available)\n",
    "        if 'satisfaction_score' in df.columns:\n",
    "            avg_satisfaction = df['satisfaction_score'].mean()\n",
    "            if avg_satisfaction >= 8:\n",
    "                summary['key_findings'].append(\"High employee satisfaction levels observed\")\n",
    "            elif avg_satisfaction >= 6:\n",
    "                summary['key_findings'].append(\"Moderate employee satisfaction levels\")\n",
    "            else:\n",
    "                summary['key_findings'].append(\"Low employee satisfaction requires attention\")\n",
    "        \n",
    "        # Department variation (if available)\n",
    "        if 'department' in df.columns and 'satisfaction_score' in df.columns:\n",
    "            dept_std = df.groupby('department')['satisfaction_score'].mean().std()\n",
    "            if dept_std > 1:\n",
    "                summary['key_findings'].append(\"Significant variation in satisfaction between departments\")\n",
    "    \n",
    "    # Generate recommendations\n",
    "    summary['recommendations'] = [\n",
    "        \"Continue monitoring key performance indicators\",\n",
    "        \"Investigate drivers of satisfaction variation\",\n",
    "        \"Implement targeted improvement programs\",\n",
    "        \"Conduct follow-up analysis to track changes\"\n",
    "    ]\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Generate executive summary\n",
    "executive_summary = generate_executive_summary(\n",
    "    loaded_data,\n",
    "    key_metrics=['satisfaction_score', 'years_experience', 'training_completed'],\n",
    "    include_insights=True\n",
    ")\n",
    "\n",
    "print(\"📋 EXECUTIVE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Analysis Date: {executive_summary['analysis_date']}\")\n",
    "print(f\"Dataset: {executive_summary['dataset_info']['name']}\")\n",
    "print(f\"Sample Size: {executive_summary['dataset_info']['sample_size']} responses\")\n",
    "\n",
    "print(\"\\n🎯 KEY FINDINGS:\")\n",
    "for i, finding in enumerate(executive_summary['key_findings'], 1):\n",
    "    print(f\"  {i}. {finding}\")\n",
    "\n",
    "print(\"\\n📊 KEY METRICS:\")\n",
    "for metric, value in executive_summary['key_metrics'].items():\n",
    "    if isinstance(value, dict):\n",
    "        print(f\"  {metric.replace('_', ' ').title()}:\")\n",
    "        for sub_metric, sub_value in value.items():\n",
    "            if isinstance(sub_value, float):\n",
    "                print(f\"    {sub_metric.replace('_', ' ').title()}: {sub_value:.2f}\")\n",
    "            else:\n",
    "                print(f\"    {sub_metric.replace('_', ' ').title()}: {sub_value}\")\n",
    "    else:\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {metric.replace('_', ' ').title()}: {value:.2f}\")\n",
    "        else:\n",
    "            print(f\"  {metric.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "print(\"\\n💡 RECOMMENDATIONS:\")\n",
    "for i, recommendation in enumerate(executive_summary['recommendations'], 1):\n",
    "    print(f\"  {i}. {recommendation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b35340a",
   "metadata": {},
   "source": [
    "## 9. Save Your Analysis\n",
    "\n",
    "Finally, let's save our analysis results for future reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8711072f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results directory\n",
    "results_dir = Path('../results')\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save analysis results\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "analysis_results = {\n",
    "    'analysis_date': datetime.now().isoformat(),\n",
    "    'dataset_info': {\n",
    "        'shape': loaded_data.shape,\n",
    "        'columns': list(loaded_data.columns)\n",
    "    },\n",
    "    'quality_assessment': quality_report,\n",
    "    'descriptive_statistics': desc_stats,\n",
    "    'correlation_analysis': correlation_result,\n",
    "    'anova_results': anova_result,\n",
    "    'executive_summary': executive_summary\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "results_file = results_dir / f'analysis_results_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json'\n",
    "\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(analysis_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"💾 Analysis results saved to: {results_file}\")\n",
    "\n",
    "# Save the current plot\n",
    "plt.savefig(results_dir / 'overview_dashboard.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"📊 Dashboard saved to: {results_dir / 'overview_dashboard.png'}\")\n",
    "\n",
    "print(\"\\n✅ Analysis complete! You've successfully:\")\n",
    "print(\"   • Loaded and explored your data\")\n",
    "print(\"   • Assessed data quality\")\n",
    "print(\"   • Generated descriptive statistics\")\n",
    "print(\"   • Created visualizations\")\n",
    "print(\"   • Performed statistical tests\")\n",
    "print(\"   • Generated an executive summary\")\n",
    "print(\"   • Saved all results for future reference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd2bb2b",
   "metadata": {},
   "source": [
    "## 🎯 Next Steps\n",
    "\n",
    "Congratulations! You've completed your first analysis with the Enterprise Data Analysis Cognitive Architecture. Here's what you can explore next:\n",
    "\n",
    "### 🚀 Advanced Notebooks\n",
    "- **02_advanced_statistics.ipynb** - Advanced statistical methods and hypothesis testing\n",
    "- **03_machine_learning.ipynb** - Predictive modeling and ML workflows\n",
    "- **04_business_intelligence.ipynb** - KPI tracking and executive dashboards\n",
    "- **05_data_visualization.ipynb** - Advanced visualization techniques\n",
    "\n",
    "### 📚 Documentation\n",
    "- **User Guide** - Comprehensive usage examples\n",
    "- **API Reference** - Detailed function documentation\n",
    "- **Architecture Guide** - Understanding the cognitive memory system\n",
    "\n",
    "### 🔧 Customization\n",
    "- Load your own data files (CSV, Excel, SPSS, databases)\n",
    "- Customize analysis parameters and visualizations\n",
    "- Extend functionality with custom plugins\n",
    "\n",
    "**Happy analyzing!** 📊✨"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
