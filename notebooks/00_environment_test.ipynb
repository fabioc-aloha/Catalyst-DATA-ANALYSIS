{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21c3c7bb",
   "metadata": {},
   "source": [
    "# Data Analysis Environment Test\n",
    "## Comprehensive Testing of New Virtual Environment\n",
    "\n",
    "This notebook tests all major components of the data analysis environment including:\n",
    "- Core data analysis libraries\n",
    "- Statistical analysis capabilities\n",
    "- SPSS integration\n",
    "- Machine learning tools\n",
    "- Visualization libraries\n",
    "- Business intelligence components\n",
    "\n",
    "**Date:** July 24, 2025  \n",
    "**Environment:** Python 3.11.9 with comprehensive data analysis stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce93abb7",
   "metadata": {},
   "source": [
    "## 1. Core Data Analysis Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bcd003e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Core libraries imported successfully!\n",
      "Pandas version: 2.3.1\n",
      "NumPy version: 2.2.6\n",
      "Matplotlib version: 3.10.3\n",
      "Seaborn version: 0.13.2\n"
     ]
    }
   ],
   "source": [
    "# Core data analysis imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Core libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Matplotlib version: {plt.matplotlib.__version__}\")\n",
    "print(f\"Seaborn version: {sns.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d5f37a",
   "metadata": {},
   "source": [
    "## 2. Statistical Analysis Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67d52439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Statistical analysis libraries imported successfully!\n",
      "Statsmodels version: 0.14.5\n",
      "Pingouin version: 0.5.5\n",
      "Factor analyzer ready\n",
      "Reliability tools available: True\n"
     ]
    }
   ],
   "source": [
    "# Statistical analysis imports\n",
    "import statsmodels.api as sm\n",
    "import pingouin as pg\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "try:\n",
    "    from reliability.Reliability_testing import Fit_Everything\n",
    "    reliability_available = True\n",
    "except ImportError:\n",
    "    try:\n",
    "        import reliability\n",
    "        reliability_available = True\n",
    "    except ImportError:\n",
    "        reliability_available = False\n",
    "\n",
    "print(\"‚úÖ Statistical analysis libraries imported successfully!\")\n",
    "print(f\"Statsmodels version: {sm.__version__}\")\n",
    "print(f\"Pingouin version: {pg.__version__}\")\n",
    "print(f\"Factor analyzer ready\")\n",
    "print(f\"Reliability tools available: {reliability_available}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7201d0",
   "metadata": {},
   "source": [
    "## 3. SPSS Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a96b8cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SPSS integration libraries imported successfully!\n",
      "Pyreadstat available: True\n",
      "SPSS savReaderWriter available: False\n",
      "SPSS files found in notebooks: ['DBA 710 Multiple Stores.sav']\n"
     ]
    }
   ],
   "source": [
    "# SPSS integration imports\n",
    "import pyreadstat\n",
    "try:\n",
    "    import savReaderWriter as spss\n",
    "    spss_available = True\n",
    "except ImportError:\n",
    "    spss_available = False\n",
    "\n",
    "print(\"‚úÖ SPSS integration libraries imported successfully!\")\n",
    "print(f\"Pyreadstat available: {pyreadstat is not None}\")\n",
    "print(f\"SPSS savReaderWriter available: {spss_available}\")\n",
    "\n",
    "# Test SPSS file reading capability\n",
    "import os\n",
    "current_dir = os.getcwd()\n",
    "notebook_dir = os.path.join(current_dir, 'notebooks') if 'notebooks' not in current_dir else current_dir\n",
    "try:\n",
    "    spss_files = [f for f in os.listdir(notebook_dir) if f.endswith('.sav')]\n",
    "    print(f\"SPSS files found in notebooks: {spss_files}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Notebooks directory not accessible from current location\")\n",
    "    print(f\"Current working directory: {current_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e2b89b",
   "metadata": {},
   "source": [
    "## 4. Machine Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5757f1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Machine learning libraries imported successfully!\n",
      "Scikit-learn available\n",
      "XGBoost version: 3.0.2\n",
      "LightGBM version: 4.6.0\n",
      "Imbalanced-learn available\n"
     ]
    }
   ],
   "source": [
    "# Machine learning imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "print(\"‚úÖ Machine learning libraries imported successfully!\")\n",
    "print(f\"Scikit-learn available\")\n",
    "print(f\"XGBoost version: {xgb.__version__}\")\n",
    "print(f\"LightGBM version: {lgb.__version__}\")\n",
    "print(\"Imbalanced-learn available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a031c9",
   "metadata": {},
   "source": [
    "## 5. Advanced Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f384851a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Advanced visualization libraries imported successfully!\n",
      "Plotly version: 6.2.0\n",
      "Bokeh version: 3.7.3\n",
      "Altair version: 5.5.0\n"
     ]
    }
   ],
   "source": [
    "# Advanced visualization imports\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly\n",
    "import bokeh\n",
    "import altair as alt\n",
    "\n",
    "print(\"‚úÖ Advanced visualization libraries imported successfully!\")\n",
    "print(f\"Plotly version: {plotly.__version__}\")\n",
    "print(f\"Bokeh version: {bokeh.__version__}\")\n",
    "print(f\"Altair version: {alt.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86303b34",
   "metadata": {},
   "source": [
    "## 6. Business Intelligence and Dashboards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef9e82bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Business intelligence libraries imported successfully!\n",
      "Dash version: 3.1.1\n",
      "Dash Bootstrap Components available\n",
      "Kaleido for static exports available\n"
     ]
    }
   ],
   "source": [
    "# Business intelligence imports\n",
    "import dash\n",
    "from dash import dcc, html\n",
    "import dash_bootstrap_components as dbc\n",
    "import kaleido  # For static image export\n",
    "\n",
    "print(\"‚úÖ Business intelligence libraries imported successfully!\")\n",
    "print(f\"Dash version: {dash.__version__}\")\n",
    "print(\"Dash Bootstrap Components available\")\n",
    "print(\"Kaleido for static exports available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d61819",
   "metadata": {},
   "source": [
    "## 7. Specialized Analysis Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36eb860f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è pmdarima import issue (numpy compatibility): numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "‚úÖ Specialized analysis tools imported successfully!\n",
      "Natural Language Processing: NLTK, TextBlob\n",
      "Geospatial Analysis: GeoPandas, Folium\n",
      "Network Analysis: NetworkX\n",
      "Time Series: ARCH\n",
      "pmdarima available: False\n"
     ]
    }
   ],
   "source": [
    "# Specialized analysis imports\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "import networkx as nx\n",
    "from arch import arch_model\n",
    "\n",
    "# Handle pmdarima numpy compatibility issue\n",
    "try:\n",
    "    import pmdarima as pm\n",
    "    pmdarima_available = True\n",
    "except (ImportError, ValueError) as e:\n",
    "    pmdarima_available = False\n",
    "    print(f\"‚ö†Ô∏è pmdarima import issue (numpy compatibility): {e}\")\n",
    "\n",
    "print(\"‚úÖ Specialized analysis tools imported successfully!\")\n",
    "print(\"Natural Language Processing: NLTK, TextBlob\")\n",
    "print(\"Geospatial Analysis: GeoPandas, Folium\")\n",
    "print(\"Network Analysis: NetworkX\")\n",
    "print(\"Time Series: ARCH\")\n",
    "print(f\"pmdarima available: {pmdarima_available}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fa2e73",
   "metadata": {},
   "source": [
    "## 8. Bayesian Analysis (Advanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43ae9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING\tpytensor.configdefaults:configdefaults.py:add_compile_configvars()- g++ not available, if using conda: `conda install gxx`\n",
      "WARNING\tpytensor.configdefaults:configdefaults.py:add_compile_configvars()- g++ not detected!  PyTensor will be unable to compile C-implementations and will default to Python. Performance may be severely degraded. To remove this warning, set PyTensor flags cxx to an empty string.\n",
      "WARNING\tpytensor.configdefaults:configdefaults.py:add_compile_configvars()- g++ not detected!  PyTensor will be unable to compile C-implementations and will default to Python. Performance may be severely degraded. To remove this warning, set PyTensor flags cxx to an empty string.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Bayesian analysis libraries imported successfully!\n",
      "PyMC version: 5.25.1\n",
      "ArviZ version: 0.22.0\n",
      "Bayesian analysis available: True\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Bayesian analysis imports\n",
    "try:\n",
    "    import pymc as pm\n",
    "    import arviz as az\n",
    "    bayesian_available = True\n",
    "    print(\"‚úÖ Bayesian analysis libraries imported successfully!\")\n",
    "    print(f\"PyMC version: {pm.__version__}\")\n",
    "    print(f\"ArviZ version: {az.__version__}\")\n",
    "except ImportError as e:\n",
    "    bayesian_available = False\n",
    "    print(f\"‚ö†Ô∏è Bayesian libraries import issue: {e}\")\n",
    "\n",
    "print(f\"Bayesian analysis available: {bayesian_available}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b04d4b",
   "metadata": {},
   "source": [
    "## 9. Test Data Creation and Basic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063974b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test dataset\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# Generate synthetic customer satisfaction data\n",
    "data = {\n",
    "    'customer_id': range(1, n_samples + 1),\n",
    "    'satisfaction_score': np.random.normal(7.5, 1.5, n_samples),\n",
    "    'service_quality': np.random.normal(7.0, 1.2, n_samples),\n",
    "    'price_satisfaction': np.random.normal(6.8, 1.8, n_samples),\n",
    "    'loyalty_intention': np.random.normal(6.5, 2.0, n_samples),\n",
    "    'age': np.random.randint(18, 75, n_samples),\n",
    "    'gender': np.random.choice(['Male', 'Female', 'Other'], n_samples),\n",
    "    'region': np.random.choice(['North', 'South', 'East', 'West'], n_samples)\n",
    "}\n",
    "\n",
    "# Ensure realistic bounds for satisfaction scores (1-10 scale)\n",
    "for col in ['satisfaction_score', 'service_quality', 'price_satisfaction', 'loyalty_intention']:\n",
    "    data[col] = np.clip(data[col], 1, 10)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"‚úÖ Test dataset created successfully!\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8c2aab",
   "metadata": {},
   "source": [
    "## 10. Basic Statistical Analysis Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d58d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic descriptive statistics\n",
    "print(\"üìä Descriptive Statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Correlation analysis\n",
    "numeric_cols = ['satisfaction_score', 'service_quality', 'price_satisfaction', 'loyalty_intention', 'age']\n",
    "correlation_matrix = df[numeric_cols].corr()\n",
    "\n",
    "print(\"\\nüîó Correlation Matrix:\")\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef68ced5",
   "metadata": {},
   "source": [
    "## 11. Visualization Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4ad6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations to test plotting capabilities\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Distribution of satisfaction scores\n",
    "axes[0, 0].hist(df['satisfaction_score'], bins=20, alpha=0.7, color='skyblue')\n",
    "axes[0, 0].set_title('Distribution of Satisfaction Scores')\n",
    "axes[0, 0].set_xlabel('Satisfaction Score')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Correlation heatmap\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Correlation Heatmap')\n",
    "\n",
    "# Satisfaction by region\n",
    "df.boxplot(column='satisfaction_score', by='region', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Satisfaction Score by Region')\n",
    "axes[1, 0].set_xlabel('Region')\n",
    "\n",
    "# Scatter plot\n",
    "axes[1, 1].scatter(df['service_quality'], df['satisfaction_score'], alpha=0.6)\n",
    "axes[1, 1].set_xlabel('Service Quality')\n",
    "axes[1, 1].set_ylabel('Satisfaction Score')\n",
    "axes[1, 1].set_title('Service Quality vs Satisfaction')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Matplotlib/Seaborn visualizations created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc26581",
   "metadata": {},
   "source": [
    "## 12. Interactive Plotly Visualization Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3f4857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive Plotly visualization\n",
    "fig = px.scatter(\n",
    "    df, \n",
    "    x='service_quality', \n",
    "    y='satisfaction_score',\n",
    "    color='region',\n",
    "    size='age',\n",
    "    hover_data=['price_satisfaction', 'loyalty_intention'],\n",
    "    title='Interactive Customer Satisfaction Analysis',\n",
    "    labels={\n",
    "        'service_quality': 'Service Quality Score',\n",
    "        'satisfaction_score': 'Overall Satisfaction Score'\n",
    "    }\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    width=800,\n",
    "    height=600,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "print(\"‚úÖ Interactive Plotly visualization created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01666b7a",
   "metadata": {},
   "source": [
    "## 13. Statistical Testing with Pingouin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79e6665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform ANOVA test to compare satisfaction across regions\n",
    "anova_result = pg.anova(data=df, dv='satisfaction_score', between='region')\n",
    "print(\"üìà ANOVA Results (Satisfaction by Region):\")\n",
    "print(anova_result)\n",
    "\n",
    "# Correlation test\n",
    "corr_result = pg.corr(df['service_quality'], df['satisfaction_score'])\n",
    "print(\"\\nüîó Correlation Test (Service Quality vs Satisfaction):\")\n",
    "print(corr_result)\n",
    "\n",
    "# Post-hoc tests if ANOVA is significant\n",
    "if anova_result['p-unc'][0] < 0.05:\n",
    "    posthoc = pg.pairwise_tukey(data=df, dv='satisfaction_score', between='region')\n",
    "    print(\"\\nüìä Post-hoc Tukey Test:\")\n",
    "    print(posthoc)\n",
    "\n",
    "print(\"\\n‚úÖ Statistical testing with Pingouin completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b7d159",
   "metadata": {},
   "source": [
    "## 14. Machine Learning Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a900bf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a classification problem: predict high vs low satisfaction\n",
    "df['high_satisfaction'] = (df['satisfaction_score'] > df['satisfaction_score'].median()).astype(int)\n",
    "\n",
    "# Prepare features\n",
    "features = ['service_quality', 'price_satisfaction', 'age']\n",
    "X = df[features]\n",
    "y = df['high_satisfaction']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "print(\"ü§ñ Machine Learning Model Performance:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': features,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nüìä Feature Importance:\")\n",
    "print(feature_importance)\n",
    "\n",
    "print(\"\\n‚úÖ Machine learning pipeline completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c460ac",
   "metadata": {},
   "source": [
    "## 15. Environment Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5375a2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment capability summary\n",
    "capabilities = {\n",
    "    'üìä Core Data Analysis': '‚úÖ Pandas, NumPy, SciPy',\n",
    "    'üìà Statistical Analysis': '‚úÖ Statsmodels, Pingouin, Factor Analysis',\n",
    "    'üîç SPSS Integration': '‚úÖ pyreadstat, savReaderWriter',\n",
    "    'ü§ñ Machine Learning': '‚úÖ scikit-learn, XGBoost, LightGBM',\n",
    "    'üìä Visualization': '‚úÖ Matplotlib, Seaborn, Plotly, Bokeh',\n",
    "    'üíº Business Intelligence': '‚úÖ Dash, Bootstrap Components',\n",
    "    'üî¨ Advanced Analytics': '‚úÖ Bayesian (PyMC), Time Series (ARCH)',\n",
    "    'üåê Specialized Tools': '‚úÖ NLP, Geospatial, Network Analysis',\n",
    "    'üìù Jupyter Environment': '‚úÖ JupyterLab, Widgets, Extensions'\n",
    "}\n",
    "\n",
    "print(\"üéØ DATA ANALYSIS ENVIRONMENT - FULLY OPERATIONAL\")\n",
    "print(\"=\" * 50)\n",
    "for capability, status in capabilities.items():\n",
    "    print(f\"{capability}: {status}\")\n",
    "\n",
    "print(\"\\nüöÄ Ready for enterprise-grade data analysis workflows!\")\n",
    "print(\"üìö All specialized notebooks and templates available\")\n",
    "print(\"üîê Security and governance protocols active\")\n",
    "print(\"‚ö° Performance optimization tools loaded\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
