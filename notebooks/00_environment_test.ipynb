{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21c3c7bb",
   "metadata": {},
   "source": [
    "# Data Analysis Environment Test\n",
    "## Comprehensive Testing of New Virtual Environment\n",
    "\n",
    "This notebook tests all major components of the data analysis environment including:\n",
    "- Core data analysis libraries\n",
    "- Statistical analysis capabilities\n",
    "- SPSS integration\n",
    "- Machine learning tools\n",
    "- Visualization libraries\n",
    "- Business intelligence components\n",
    "\n",
    "**Date:** July 24, 2025  \n",
    "**Environment:** Python 3.11.9 with comprehensive data analysis stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce93abb7",
   "metadata": {},
   "source": [
    "## 1. Core Data Analysis Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bcd003e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Core libraries imported successfully!\n",
      "Pandas version: 2.3.1\n",
      "NumPy version: 2.2.6\n",
      "Matplotlib version: 3.10.3\n",
      "Seaborn version: 0.13.2\n"
     ]
    }
   ],
   "source": [
    "# Core data analysis imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✅ Core libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Matplotlib version: {plt.matplotlib.__version__}\")\n",
    "print(f\"Seaborn version: {sns.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d5f37a",
   "metadata": {},
   "source": [
    "## 2. Statistical Analysis Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67d52439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Statistical analysis libraries imported successfully!\n",
      "Statsmodels version: 0.14.5\n",
      "Pingouin version: 0.5.5\n",
      "Factor analyzer ready\n",
      "Reliability tools available: True\n"
     ]
    }
   ],
   "source": [
    "# Statistical analysis imports\n",
    "import statsmodels.api as sm\n",
    "import pingouin as pg\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "try:\n",
    "    from reliability.Reliability_testing import Fit_Everything\n",
    "    reliability_available = True\n",
    "except ImportError:\n",
    "    try:\n",
    "        import reliability\n",
    "        reliability_available = True\n",
    "    except ImportError:\n",
    "        reliability_available = False\n",
    "\n",
    "print(\"✅ Statistical analysis libraries imported successfully!\")\n",
    "print(f\"Statsmodels version: {sm.__version__}\")\n",
    "print(f\"Pingouin version: {pg.__version__}\")\n",
    "print(f\"Factor analyzer ready\")\n",
    "print(f\"Reliability tools available: {reliability_available}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7201d0",
   "metadata": {},
   "source": [
    "## 3. SPSS Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a96b8cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SPSS integration libraries imported successfully!\n",
      "Pyreadstat available: True\n",
      "SPSS savReaderWriter available: False\n",
      "SPSS files found in notebooks: ['DBA 710 Multiple Stores.sav']\n"
     ]
    }
   ],
   "source": [
    "# SPSS integration imports\n",
    "import pyreadstat\n",
    "try:\n",
    "    import savReaderWriter as spss\n",
    "    spss_available = True\n",
    "except ImportError:\n",
    "    spss_available = False\n",
    "\n",
    "print(\"✅ SPSS integration libraries imported successfully!\")\n",
    "print(f\"Pyreadstat available: {pyreadstat is not None}\")\n",
    "print(f\"SPSS savReaderWriter available: {spss_available}\")\n",
    "\n",
    "# Test SPSS file reading capability\n",
    "import os\n",
    "current_dir = os.getcwd()\n",
    "notebook_dir = os.path.join(current_dir, 'notebooks') if 'notebooks' not in current_dir else current_dir\n",
    "try:\n",
    "    spss_files = [f for f in os.listdir(notebook_dir) if f.endswith('.sav')]\n",
    "    print(f\"SPSS files found in notebooks: {spss_files}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Notebooks directory not accessible from current location\")\n",
    "    print(f\"Current working directory: {current_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e2b89b",
   "metadata": {},
   "source": [
    "## 4. Machine Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5757f1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Machine learning libraries imported successfully!\n",
      "Scikit-learn available\n",
      "XGBoost version: 3.0.2\n",
      "LightGBM version: 4.6.0\n",
      "Imbalanced-learn available\n"
     ]
    }
   ],
   "source": [
    "# Machine learning imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "print(\"✅ Machine learning libraries imported successfully!\")\n",
    "print(f\"Scikit-learn available\")\n",
    "print(f\"XGBoost version: {xgb.__version__}\")\n",
    "print(f\"LightGBM version: {lgb.__version__}\")\n",
    "print(\"Imbalanced-learn available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a031c9",
   "metadata": {},
   "source": [
    "## 5. Advanced Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f384851a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Advanced visualization libraries imported successfully!\n",
      "Plotly version: 6.2.0\n",
      "Bokeh version: 3.7.3\n",
      "Altair version: 5.5.0\n"
     ]
    }
   ],
   "source": [
    "# Advanced visualization imports\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly\n",
    "import bokeh\n",
    "import altair as alt\n",
    "\n",
    "print(\"✅ Advanced visualization libraries imported successfully!\")\n",
    "print(f\"Plotly version: {plotly.__version__}\")\n",
    "print(f\"Bokeh version: {bokeh.__version__}\")\n",
    "print(f\"Altair version: {alt.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86303b34",
   "metadata": {},
   "source": [
    "## 6. Business Intelligence and Dashboards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef9e82bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Business intelligence libraries imported successfully!\n",
      "Dash version: 3.1.1\n",
      "Dash Bootstrap Components available\n",
      "Kaleido for static exports available\n"
     ]
    }
   ],
   "source": [
    "# Business intelligence imports\n",
    "import dash\n",
    "from dash import dcc, html\n",
    "import dash_bootstrap_components as dbc\n",
    "import kaleido  # For static image export\n",
    "\n",
    "print(\"✅ Business intelligence libraries imported successfully!\")\n",
    "print(f\"Dash version: {dash.__version__}\")\n",
    "print(\"Dash Bootstrap Components available\")\n",
    "print(\"Kaleido for static exports available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d61819",
   "metadata": {},
   "source": [
    "## 7. Specialized Analysis Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36eb860f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ pmdarima import issue (numpy compatibility): numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "✅ Specialized analysis tools imported successfully!\n",
      "Natural Language Processing: NLTK, TextBlob\n",
      "Geospatial Analysis: GeoPandas, Folium\n",
      "Network Analysis: NetworkX\n",
      "Time Series: ARCH\n",
      "pmdarima available: False\n"
     ]
    }
   ],
   "source": [
    "# Specialized analysis imports\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "import networkx as nx\n",
    "from arch import arch_model\n",
    "\n",
    "# Handle pmdarima numpy compatibility issue\n",
    "try:\n",
    "    import pmdarima as pm\n",
    "    pmdarima_available = True\n",
    "except (ImportError, ValueError) as e:\n",
    "    pmdarima_available = False\n",
    "    print(f\"⚠️ pmdarima import issue (numpy compatibility): {e}\")\n",
    "\n",
    "print(\"✅ Specialized analysis tools imported successfully!\")\n",
    "print(\"Natural Language Processing: NLTK, TextBlob\")\n",
    "print(\"Geospatial Analysis: GeoPandas, Folium\")\n",
    "print(\"Network Analysis: NetworkX\")\n",
    "print(\"Time Series: ARCH\")\n",
    "print(f\"pmdarima available: {pmdarima_available}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fa2e73",
   "metadata": {},
   "source": [
    "## 8. Bayesian Analysis (Advanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43ae9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING\tpytensor.configdefaults:configdefaults.py:add_compile_configvars()- g++ not available, if using conda: `conda install gxx`\n",
      "WARNING\tpytensor.configdefaults:configdefaults.py:add_compile_configvars()- g++ not detected!  PyTensor will be unable to compile C-implementations and will default to Python. Performance may be severely degraded. To remove this warning, set PyTensor flags cxx to an empty string.\n",
      "WARNING\tpytensor.configdefaults:configdefaults.py:add_compile_configvars()- g++ not detected!  PyTensor will be unable to compile C-implementations and will default to Python. Performance may be severely degraded. To remove this warning, set PyTensor flags cxx to an empty string.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Bayesian analysis libraries imported successfully!\n",
      "PyMC version: 5.25.1\n",
      "ArviZ version: 0.22.0\n",
      "Bayesian analysis available: True\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Bayesian analysis imports\n",
    "try:\n",
    "    import pymc as pm\n",
    "    import arviz as az\n",
    "    bayesian_available = True\n",
    "    print(\"✅ Bayesian analysis libraries imported successfully!\")\n",
    "    print(f\"PyMC version: {pm.__version__}\")\n",
    "    print(f\"ArviZ version: {az.__version__}\")\n",
    "except ImportError as e:\n",
    "    bayesian_available = False\n",
    "    print(f\"⚠️ Bayesian libraries import issue: {e}\")\n",
    "\n",
    "print(f\"Bayesian analysis available: {bayesian_available}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b04d4b",
   "metadata": {},
   "source": [
    "## 9. Test Data Creation and Basic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063974b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test dataset\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# Generate synthetic customer satisfaction data\n",
    "data = {\n",
    "    'customer_id': range(1, n_samples + 1),\n",
    "    'satisfaction_score': np.random.normal(7.5, 1.5, n_samples),\n",
    "    'service_quality': np.random.normal(7.0, 1.2, n_samples),\n",
    "    'price_satisfaction': np.random.normal(6.8, 1.8, n_samples),\n",
    "    'loyalty_intention': np.random.normal(6.5, 2.0, n_samples),\n",
    "    'age': np.random.randint(18, 75, n_samples),\n",
    "    'gender': np.random.choice(['Male', 'Female', 'Other'], n_samples),\n",
    "    'region': np.random.choice(['North', 'South', 'East', 'West'], n_samples)\n",
    "}\n",
    "\n",
    "# Ensure realistic bounds for satisfaction scores (1-10 scale)\n",
    "for col in ['satisfaction_score', 'service_quality', 'price_satisfaction', 'loyalty_intention']:\n",
    "    data[col] = np.clip(data[col], 1, 10)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"✅ Test dataset created successfully!\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8c2aab",
   "metadata": {},
   "source": [
    "## 10. Basic Statistical Analysis Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d58d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic descriptive statistics\n",
    "print(\"📊 Descriptive Statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Correlation analysis\n",
    "numeric_cols = ['satisfaction_score', 'service_quality', 'price_satisfaction', 'loyalty_intention', 'age']\n",
    "correlation_matrix = df[numeric_cols].corr()\n",
    "\n",
    "print(\"\\n🔗 Correlation Matrix:\")\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef68ced5",
   "metadata": {},
   "source": [
    "## 11. Visualization Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4ad6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations to test plotting capabilities\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Distribution of satisfaction scores\n",
    "axes[0, 0].hist(df['satisfaction_score'], bins=20, alpha=0.7, color='skyblue')\n",
    "axes[0, 0].set_title('Distribution of Satisfaction Scores')\n",
    "axes[0, 0].set_xlabel('Satisfaction Score')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Correlation heatmap\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Correlation Heatmap')\n",
    "\n",
    "# Satisfaction by region\n",
    "df.boxplot(column='satisfaction_score', by='region', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Satisfaction Score by Region')\n",
    "axes[1, 0].set_xlabel('Region')\n",
    "\n",
    "# Scatter plot\n",
    "axes[1, 1].scatter(df['service_quality'], df['satisfaction_score'], alpha=0.6)\n",
    "axes[1, 1].set_xlabel('Service Quality')\n",
    "axes[1, 1].set_ylabel('Satisfaction Score')\n",
    "axes[1, 1].set_title('Service Quality vs Satisfaction')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Matplotlib/Seaborn visualizations created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc26581",
   "metadata": {},
   "source": [
    "## 12. Interactive Plotly Visualization Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3f4857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive Plotly visualization\n",
    "fig = px.scatter(\n",
    "    df, \n",
    "    x='service_quality', \n",
    "    y='satisfaction_score',\n",
    "    color='region',\n",
    "    size='age',\n",
    "    hover_data=['price_satisfaction', 'loyalty_intention'],\n",
    "    title='Interactive Customer Satisfaction Analysis',\n",
    "    labels={\n",
    "        'service_quality': 'Service Quality Score',\n",
    "        'satisfaction_score': 'Overall Satisfaction Score'\n",
    "    }\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    width=800,\n",
    "    height=600,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "print(\"✅ Interactive Plotly visualization created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01666b7a",
   "metadata": {},
   "source": [
    "## 13. Statistical Testing with Pingouin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79e6665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform ANOVA test to compare satisfaction across regions\n",
    "anova_result = pg.anova(data=df, dv='satisfaction_score', between='region')\n",
    "print(\"📈 ANOVA Results (Satisfaction by Region):\")\n",
    "print(anova_result)\n",
    "\n",
    "# Correlation test\n",
    "corr_result = pg.corr(df['service_quality'], df['satisfaction_score'])\n",
    "print(\"\\n🔗 Correlation Test (Service Quality vs Satisfaction):\")\n",
    "print(corr_result)\n",
    "\n",
    "# Post-hoc tests if ANOVA is significant\n",
    "if anova_result['p-unc'][0] < 0.05:\n",
    "    posthoc = pg.pairwise_tukey(data=df, dv='satisfaction_score', between='region')\n",
    "    print(\"\\n📊 Post-hoc Tukey Test:\")\n",
    "    print(posthoc)\n",
    "\n",
    "print(\"\\n✅ Statistical testing with Pingouin completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b7d159",
   "metadata": {},
   "source": [
    "## 14. Machine Learning Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a900bf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a classification problem: predict high vs low satisfaction\n",
    "df['high_satisfaction'] = (df['satisfaction_score'] > df['satisfaction_score'].median()).astype(int)\n",
    "\n",
    "# Prepare features\n",
    "features = ['service_quality', 'price_satisfaction', 'age']\n",
    "X = df[features]\n",
    "y = df['high_satisfaction']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "print(\"🤖 Machine Learning Model Performance:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': features,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\n📊 Feature Importance:\")\n",
    "print(feature_importance)\n",
    "\n",
    "print(\"\\n✅ Machine learning pipeline completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c460ac",
   "metadata": {},
   "source": [
    "## 15. Environment Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5375a2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment capability summary\n",
    "capabilities = {\n",
    "    '📊 Core Data Analysis': '✅ Pandas, NumPy, SciPy',\n",
    "    '📈 Statistical Analysis': '✅ Statsmodels, Pingouin, Factor Analysis',\n",
    "    '🔍 SPSS Integration': '✅ pyreadstat, savReaderWriter',\n",
    "    '🤖 Machine Learning': '✅ scikit-learn, XGBoost, LightGBM',\n",
    "    '📊 Visualization': '✅ Matplotlib, Seaborn, Plotly, Bokeh',\n",
    "    '💼 Business Intelligence': '✅ Dash, Bootstrap Components',\n",
    "    '🔬 Advanced Analytics': '✅ Bayesian (PyMC), Time Series (ARCH)',\n",
    "    '🌐 Specialized Tools': '✅ NLP, Geospatial, Network Analysis',\n",
    "    '📝 Jupyter Environment': '✅ JupyterLab, Widgets, Extensions'\n",
    "}\n",
    "\n",
    "print(\"🎯 DATA ANALYSIS ENVIRONMENT - FULLY OPERATIONAL\")\n",
    "print(\"=\" * 50)\n",
    "for capability, status in capabilities.items():\n",
    "    print(f\"{capability}: {status}\")\n",
    "\n",
    "print(\"\\n🚀 Ready for enterprise-grade data analysis workflows!\")\n",
    "print(\"📚 All specialized notebooks and templates available\")\n",
    "print(\"🔐 Security and governance protocols active\")\n",
    "print(\"⚡ Performance optimization tools loaded\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
