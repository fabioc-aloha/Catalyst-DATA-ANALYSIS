# Enterprise Data Analysis & Business Intelligence Cognitive Architecture Auto-Setup Guide

ðŸ“– **USER MANUAL AVAILABLE** - [Complete step-by-step guide](MANUAL-DATA-ANALYSIS.md) for Enterprise Data Analysis cognitive architecture mastery

[![Version](https://img.shields.io/badge/Version-0.9.0%20THORIUM-lightgray?style=for-the-badge&logo=atom&logoColor=white)](VERSION-HISTORY.md) [![Meta Meta](https://img.shields.io/badge/Meta_Meta_Cognition-Universal_Generator-purple?style=for-the-badge&logo=brain&logoColor=white)](SETUP-META-META-COGNITION.md) [![Enterprise](https://img.shields.io/badge/Enterprise_Framework-9_Point_Integration-gold?style=for-the-badge&logo=verified&logoColor=white)](IMPLEMENTATION_GUIDE.md) [![Research](https://img.shields.io/badge/Academic_Validation-DBA_Research-blue?style=for-the-badge&logo=university&logoColor=white)](SETUP-ACADEMIC.md) [![Data Analysis](https://img.shields.io/badge/Data_Analysis-ENTERPRISE_ENHANCED-green?style=for-the-badge&logo=chartline&logoColor=white)](#) [![Dual Context](https://img.shields.io/badge/Dual_Context-Framework_Integrated-orange?style=for-the-badge&logo=microsoft&logoColor=white)](#)

## ðŸ§  Enterprise Data Analysis & Business Intelligence Excellence

**Advanced Enterprise Framework**: Institutional-grade data analysis and business intelligence with comprehensive security, automated analytics, enterprise data governance, scalable BI solutions, and cognitive architecture optimization for professional data science excellence.

**9-Point Enterprise Methodology Integration**:
1. **Empirical Integrity** â†’ Accurate data analysis FIRST, then conservative presentation with verified insights
2. **Security Excellence** â†’ Data encryption, access controls, privacy compliance, secure data handling protocols
3. **Testing & Validation** â†’ Automated data validation, statistical testing, model validation, analytics QA frameworks
4. **Automation Efficiency** â†’ Automated reporting, ETL pipelines, scheduled analytics, BI automation workflows
5. **Risk Management** â†’ Data quality monitoring, bias detection, model risk management, compliance tracking
6. **Compliance Governance** â†’ GDPR, HIPAA, SOX compliance, data governance, comprehensive audit trails
7. **Performance Optimization** â†’ Big data processing, distributed computing, analytics performance optimization
8. **Analytics Intelligence** â†’ Real-time dashboards, business intelligence, predictive analytics, ML integration
9. **Quality Assurance** â†’ Data quality frameworks, statistical validation, enterprise reporting standards

**Microsoft GCX Implementation Context**: [Microsoft Internal] This setup guide supports the Microsoft Global Customer Experience (GCX) cognitive architecture deployment, providing data analysis validation through enterprise analytics methodology for customer data platforms, business intelligence solutions, predictive analytics systems, and compliance-critical data science applications. The XODO team pilot program (July-August 2025) validates data analysis frameworks across customer experience analytics, with full GCX rollout planned for September-October 2025.

**Universal Implementation Context**: [External Organizations] This setup guide provides comprehensive data analysis cognitive architecture guidance for any organization, university, or professional context. The framework adapts to diverse data environments, analytical requirements, and organizational standards while maintaining cognitive science compliance and professional data science excellence.

**Dual-Context Adaptability**: The cognitive architecture automatically detects deployment context (Microsoft internal vs. external) and adapts data analysis practices, security protocols, analytics frameworks, and integration approaches accordingly. Microsoft-specific features (GCX alignment, enterprise integration) activate for internal scenarios, while universal frameworks (organizational adaptation, flexible compliance) engage for external implementations.

**Empirical Integrity Foundation**: Built on rigorous data analysis standards with enhanced methodological validation. This framework follows the core principle: "accurate analysis FIRST, then conservative presentation" while preserving verified analytical insights. All data analysis processes incorporate quantitative verification techniques and systematic accuracy protocols to ensure research-grade reliability for academic publication and business intelligence validation.

**Academic Timeline Integration**: Supports August 15, 2025 publication deadline for DBA research submission, with ongoing empirical validation through Microsoft GCX implementation providing real-world evidence of data analysis methodology effectiveness in enterprise analytics environments.

**Academic Validation Timeline**:
- **August 15, 2025**: Academic publication deadline (Touro University DBA)
- **September-October 2025**: Data analysis methodology validation study through Microsoft GCX deployment
- **December 15, 2025**: DBA project completion and defense

**Enhancement Framework**: 9-point enterprise methodology integration ensuring empirical integrity, security excellence, testing validation, automation efficiency, risk management, compliance governance, performance optimization, analytics intelligence, and quality assurance systems for institutional-grade data analysis capabilities.

**INSTRUCTIONS FOR GITHUB COPILOT**: This file contains complete setup instructions for implementing the Project Catalyst cognitive memory architecture optimized for Enterprise Data Analysis and Business Intelligence with institutional-grade analytics, security, and governance. Follow these instructions to create all necessary directories, files, and configurations.

## ðŸ“Š Enterprise Data Analysis Automated Setup Protocol

**Execute these steps in order to establish a complete enterprise cognitive architecture system for advanced business intelligence and statistical data analysis:**

### Step 1: Enterprise VS Code Configuration (Required)

Add these enterprise-grade settings to VS Code `settings.json`:

```json
{
  `"github.copilot.chat.codeGeneration.useInstructionFiles`": true,
  `"chat.promptFiles`": true,
  `"chat.instructionsFilesLocations`": [`".github/instructions`"],
  
  // Enterprise Python & Analytics Configuration
  `"python.defaultInterpreterPath`": `".venv/Scripts/python.exe`",
  `"python.envFile`": `"${workspaceFolder}/.env`",
  `"python.terminal.activateEnvironment`": true,
  `"python.terminal.activateEnvInCurrentTerminal`": true,
  
  // Enterprise Code Quality & Security
  `"python.linting.enabled`": true,
  `"python.linting.pylintEnabled`": true,
  `"python.linting.flake8Enabled`": true,
  `"python.linting.mypyEnabled`": true,
  `"python.linting.banditEnabled`": true,
  `"python.formatting.provider`": `"black`",
  `"python.formatting.blackArgs`": [`"--line-length=88`", `"--target-version=py311`"],
  `"python.sortImports.args`": [`"--profile=black`"],
  
  // Enterprise Testing & Validation
  `"python.testing.pytestEnabled`": true,
  `"python.testing.pytestArgs`": [`"tests/`", `"--cov=src`", `"--cov-report=html`"],
  `"python.testing.autoTestDiscoverOnSaveEnabled`": true,
  
  // Enterprise Jupyter & Analytics
  `"jupyter.askForKernelRestart`": false,
  `"jupyter.alwaysTrustNotebooks`": true,
  `"jupyter.showCellInputCode`": true,
  `"jupyter.interactiveWindow.textEditor.executeSelection`": true,
  `"jupyter.enableCellCodeLens`": true,
  `"jupyter.sendSelectionToInteractiveWindow`": true,
  `"jupyter.variableExplorerExclude`": `"module;function;builtin_function_or_method`",
  `"jupyter.debugJustMyCode`": false,
  
  // Enterprise Data Security & Compliance
  `"files.associations`": {
    `"*.py`": `"python`",
    `"*.ipynb`": `"jupyter-notebook`",
    `"*.sav`": `"spss`",
    `"*.dta`": `"stata`",
    `"*.csv`": `"csv`",
    `"*.tsv`": `"tsv`",
    `"*.json`": `"json`",
    `"*.parquet`": `"binary`",
    `"*.h5`": `"binary`",
    `"*.hdf5`": `"binary`",
    `"requirements*.txt`": `"pip-requirements`",
    `"pyproject.toml`": `"toml`",
    `"*.sql`": `"sql`",
    `"*.yaml`": `"yaml`",
    `"*.yml`": `"yaml`"
  },
  
  // Enterprise Notebook Configuration
  `"notebook.cellToolbarLocation`": {
    `"default`": `"right`",
    `"jupyter-notebook`": `"left`"
  },
  `"notebook.formatOnSave.enabled`": true,
  `"notebook.codeActionsOnSave`": {
    `"source.organizeImports`": true
  },
  
  // Enterprise Security & Privacy
  `"files.exclude`": {
    `"**/__pycache__`": true,
    `"**/*.pyc`": true,
    `"**/.coverage`": true,
    `"**/htmlcov`": true,
    `"**/.pytest_cache`": true,
    `"**/.mypy_cache`": true,
    `"**/data/sensitive/`": true,
    `"**/data/pii/`": true,
    `"**/.env`": true,
    `"**/.env.local`": true
  },
  
  // Enterprise Performance & Analytics
  `"python.analysis.typeCheckingMode`": `"strict`",
  `"python.analysis.autoImportCompletions`": true,
  `"python.analysis.diagnosticMode`": `"workspace`",
  `"editor.codeActionsOnSave`": {
    `"source.organizeImports`": true,
    `"source.fixAll`": true
  },
  `"editor.formatOnSave`": true,
  
  // Enterprise Extensions Integration
  `"sqltools.connections`": [],
  `"powerquery.enablePreview`": true,
  `"excel.enableDatabinding`": true
}
```

**Access settings.json**: `Ctrl+Shift+P` â†’ `"Preferences: Open User Settings (JSON)`"

### Step 2: Create Enterprise Directory Structure

Create this comprehensive enterprise folder structure in the project root:

```
project-root/
â”œâ”€â”€ .github/
â”‚   â”œâ”€â”€ copilot-instructions.md          # Global Declarative Memory
â”‚   â”œâ”€â”€ instructions/                    # Procedural Memory Store (45 files)
â”‚   â”‚   â”œâ”€â”€ data-analysis.instructions.md
â”‚   â”‚   â”œâ”€â”€ enterprise-analytics.instructions.md
â”‚   â”‚   â”œâ”€â”€ statistical-methods.instructions.md
â”‚   â”‚   â”œâ”€â”€ business-intelligence.instructions.md
â”‚   â”‚   â”œâ”€â”€ data-visualization.instructions.md
â”‚   â”‚   â”œâ”€â”€ machine-learning.instructions.md
â”‚   â”‚   â”œâ”€â”€ data-engineering.instructions.md
â”‚   â”‚   â”œâ”€â”€ etl-pipelines.instructions.md
â”‚   â”‚   â”œâ”€â”€ data-governance.instructions.md
â”‚   â”‚   â”œâ”€â”€ data-security.instructions.md
â”‚   â”‚   â”œâ”€â”€ privacy-compliance.instructions.md
â”‚   â”‚   â”œâ”€â”€ quality-assurance.instructions.md
â”‚   â”‚   â”œâ”€â”€ performance-optimization.instructions.md
â”‚   â”‚   â”œâ”€â”€ big-data.instructions.md
â”‚   â”‚   â”œâ”€â”€ cloud-analytics.instructions.md
â”‚   â”‚   â”œâ”€â”€ real-time-analytics.instructions.md
â”‚   â”‚   â”œâ”€â”€ predictive-modeling.instructions.md
â”‚   â”‚   â”œâ”€â”€ time-series.instructions.md
â”‚   â”‚   â”œâ”€â”€ experimental-design.instructions.md
â”‚   â”‚   â”œâ”€â”€ hypothesis-testing.instructions.md
â”‚   â”‚   â”œâ”€â”€ regression-analysis.instructions.md
â”‚   â”‚   â”œâ”€â”€ clustering-analysis.instructions.md
â”‚   â”‚   â”œâ”€â”€ classification.instructions.md
â”‚   â”‚   â”œâ”€â”€ dimensionality-reduction.instructions.md
â”‚   â”‚   â”œâ”€â”€ text-analytics.instructions.md
â”‚   â”‚   â”œâ”€â”€ network-analysis.instructions.md
â”‚   â”‚   â”œâ”€â”€ geospatial-analysis.instructions.md
â”‚   â”‚   â”œâ”€â”€ survey-analysis.instructions.md
â”‚   â”‚   â”œâ”€â”€ market-research.instructions.md
â”‚   â”‚   â”œâ”€â”€ financial-analytics.instructions.md
â”‚   â”‚   â”œâ”€â”€ risk-analytics.instructions.md
â”‚   â”‚   â”œâ”€â”€ reporting-automation.instructions.md
â”‚   â”‚   â”œâ”€â”€ dashboard-development.instructions.md
â”‚   â”‚   â”œâ”€â”€ data-storytelling.instructions.md
â”‚   â”‚   â”œâ”€â”€ statistical-software.instructions.md
â”‚   â”‚   â”œâ”€â”€ database-analytics.instructions.md
â”‚   â”‚   â”œâ”€â”€ api-integration.instructions.md
â”‚   â”‚   â”œâ”€â”€ model-deployment.instructions.md
â”‚   â”‚   â”œâ”€â”€ team-collaboration.instructions.md
â”‚   â”‚   â”œâ”€â”€ enterprise-standards.instructions.md
â”‚   â”‚   â”œâ”€â”€ cognitive-optimization.instructions.md
â”‚   â”‚   â”œâ”€â”€ workflow-automation.instructions.md
â”‚   â”‚   â”œâ”€â”€ data-lineage.instructions.md
â”‚   â”‚   â”œâ”€â”€ model-monitoring.instructions.md
â”‚   â”‚   â””â”€â”€ enterprise-integration.instructions.md
â”‚   â””â”€â”€ prompts/                         # Episodic Memory Store (45 files)
â”‚       â”œâ”€â”€ data-exploration.prompt.md
â”‚       â”œâ”€â”€ enterprise-reporting.prompt.md
â”‚       â”œâ”€â”€ statistical-analysis.prompt.md
â”‚       â”œâ”€â”€ business-insights.prompt.md
â”‚       â”œâ”€â”€ data-cleaning.prompt.md
â”‚       â”œâ”€â”€ feature-engineering.prompt.md
â”‚       â”œâ”€â”€ model-building.prompt.md
â”‚       â”œâ”€â”€ model-validation.prompt.md
â”‚       â”œâ”€â”€ performance-tuning.prompt.md
â”‚       â”œâ”€â”€ data-pipeline.prompt.md
â”‚       â”œâ”€â”€ etl-development.prompt.md
â”‚       â”œâ”€â”€ dashboard-creation.prompt.md
â”‚       â”œâ”€â”€ visualization-design.prompt.md
â”‚       â”œâ”€â”€ anomaly-detection.prompt.md
â”‚       â”œâ”€â”€ forecasting.prompt.md
â”‚       â”œâ”€â”€ customer-analytics.prompt.md
â”‚       â”œâ”€â”€ marketing-analytics.prompt.md
â”‚       â”œâ”€â”€ sales-analytics.prompt.md
â”‚       â”œâ”€â”€ financial-modeling.prompt.md
â”‚       â”œâ”€â”€ risk-assessment.prompt.md
â”‚       â”œâ”€â”€ compliance-reporting.prompt.md
â”‚       â”œâ”€â”€ data-governance.prompt.md
â”‚       â”œâ”€â”€ quality-monitoring.prompt.md
â”‚       â”œâ”€â”€ security-audit.prompt.md
â”‚       â”œâ”€â”€ privacy-assessment.prompt.md
â”‚       â”œâ”€â”€ experimental-design.prompt.md
â”‚       â”œâ”€â”€ ab-testing.prompt.md
â”‚       â”œâ”€â”€ survey-design.prompt.md
â”‚       â”œâ”€â”€ research-methodology.prompt.md
â”‚       â”œâ”€â”€ data-integration.prompt.md
â”‚       â”œâ”€â”€ database-optimization.prompt.md
â”‚       â”œâ”€â”€ cloud-migration.prompt.md
â”‚       â”œâ”€â”€ scalability-planning.prompt.md
â”‚       â”œâ”€â”€ automation-workflow.prompt.md
â”‚       â”œâ”€â”€ team-training.prompt.md
â”‚       â”œâ”€â”€ project-planning.prompt.md
â”‚       â”œâ”€â”€ stakeholder-communication.prompt.md
â”‚       â”œâ”€â”€ enterprise-architecture.prompt.md
â”‚       â”œâ”€â”€ cognitive-assessment.prompt.md
â”‚       â”œâ”€â”€ workflow-optimization.prompt.md
â”‚       â”œâ”€â”€ innovation-discovery.prompt.md
â”‚       â”œâ”€â”€ strategic-planning.prompt.md
â”‚       â”œâ”€â”€ competitive-analysis.prompt.md
â”‚       â”œâ”€â”€ consolidation.prompt.md
â”‚       â””â”€â”€ self-assessment.prompt.md
â”œâ”€â”€ .venv/                               # Virtual Environment
â”œâ”€â”€ notebooks/                           # Jupyter Notebooks
â”‚   â”œâ”€â”€ exploratory/                     # Exploratory Analysis
â”‚   â”œâ”€â”€ modeling/                        # Statistical Modeling
â”‚   â”œâ”€â”€ visualization/                   # Data Visualization
â”‚   â””â”€â”€ reports/                         # Final Reports
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                            # Raw Data Files
â”‚   â”œâ”€â”€ processed/                      # Cleaned Data
â”‚   â”œâ”€â”€ output/                         # Analysis Results
â”‚   â”œâ”€â”€ sensitive/                      # Sensitive Data (excluded from git)
â”‚   â””â”€â”€ pii/                           # PII Data (excluded from git)
â”œâ”€â”€ src/                                # Source Code
â”‚   â”œâ”€â”€ utils/                          # Utility Functions
â”‚   â”œâ”€â”€ models/                         # Statistical Models
â”‚   â”œâ”€â”€ visualization/                  # Custom Plots
â”‚   â”œâ”€â”€ pipelines/                      # Data Pipelines
â”‚   â””â”€â”€ validation/                     # Data Validation
â”œâ”€â”€ tests/                              # Test Files
â”œâ”€â”€ docs/                               # Documentation
â”œâ”€â”€ config/                             # Configuration Files
â”œâ”€â”€ requirements.txt                    # Core Dependencies
â”œâ”€â”€ requirements-analysis.txt           # Analysis-specific Dependencies
â”œâ”€â”€ requirements-dev.txt                # Development Dependencies
â”œâ”€â”€ .env                                # Environment Variables
â”œâ”€â”€ .gitignore                         # Git Ignore
â””â”€â”€ pyproject.toml                     # Project Configuration
```
```

### Step 3: Advanced Analytics Environment Setup

**Create virtual environment and install comprehensive analytics stack:**

```powershell
# Create virtual environment
python -m venv .venv

# Activate virtual environment (Windows PowerShell)
.venv\Scripts\Activate.ps1

# Upgrade pip and install build tools
python -m pip install --upgrade pip setuptools wheel

# Create project directories
New-Item -ItemType Directory -Force -Path `"notebooks/exploratory`", `"notebooks/modeling`", `"notebooks/visualization`", `"notebooks/reports`"
New-Item -ItemType Directory -Force -Path `"data/raw`", `"data/processed`", `"data/output`"
New-Item -ItemType Directory -Force -Path `"src/utils`", `"src/models`", `"src/visualization`", `"tests`"

# Create comprehensive requirements files
@`"
# Core Data Analysis Libraries
pandas>=2.1.0
numpy>=1.24.0
scipy>=1.11.0

# SPSS Data Processing
pyreadstat>=1.2.0
savReaderWriter>=4.0.0

# Statistical Analysis
statsmodels>=0.14.0
pingouin>=0.5.3
scikit-learn>=1.3.0
lifelines>=0.27.0

# Advanced Analytics
factor-analyzer>=0.4.1
prince>=0.7.1
imbalanced-learn>=0.11.0
optuna>=3.3.0

# Visualization
matplotlib>=3.7.0
seaborn>=0.12.0
plotly>=5.15.0
bokeh>=3.2.0
altair>=5.0.0

# Data Processing
openpyxl>=3.1.0
xlsxwriter>=3.1.0
python-dotenv>=1.0.0
tqdm>=4.65.0
`"@ | Out-File -FilePath `"requirements.txt`" -Encoding utf8

@`"
# Advanced Statistical Libraries
rpy2>=3.5.13
pymc>=5.7.0
arviz>=0.16.0
bambi>=0.12.0
xarray>=2023.7.0

# Machine Learning Extensions
xgboost>=1.7.0
lightgbm>=4.0.0
catboost>=1.2.0
tensorflow>=2.13.0
torch>=2.0.0
transformers>=4.33.0

# Time Series Analysis
prophet>=1.1.4
sktime>=0.21.0
darts>=0.25.0
neuralprophet>=0.5.4

# Network Analysis
networkx>=3.1.0
pyvis>=0.3.2

# Text Analytics
nltk>=3.8.0
spacy>=3.6.0
textblob>=0.17.0
wordcloud>=1.9.0

# Geospatial Analysis
geopandas>=0.13.0
folium>=0.14.0
contextily>=1.3.0

# Dimensionality Reduction
umap-learn>=0.5.3
hdbscan>=0.8.29
`"@ | Out-File -FilePath `"requirements-analysis.txt`" -Encoding utf8

@`"
# Development and Testing
jupyter>=1.0.0
jupyterlab>=4.0.0
notebook>=7.0.0
ipywidgets>=8.1.0
jupytext>=1.15.0

# Code Quality
pytest>=7.4.0
pytest-cov>=4.1.0
black>=23.0.0
flake8>=6.0.0
pylint>=2.17.0
mypy>=1.5.0
isort>=5.12.0

# Documentation
sphinx>=7.1.0
sphinx-rtd-theme>=1.3.0
nbsphinx>=0.9.0
jupyter-book>=0.15.0

# Performance Profiling
memory-profiler>=0.61.0
line-profiler>=4.1.0
py-spy>=0.3.14

# Data Validation
great-expectations>=0.17.0
pandera>=0.15.0
cerberus>=1.3.4
`"@ | Out-File -FilePath `"requirements-dev.txt`" -Encoding utf8

# Install all dependencies
Write-Host `"Installing core dependencies...`" -ForegroundColor Green
pip install -r requirements.txt

Write-Host `"Installing analysis-specific dependencies...`" -ForegroundColor Green
pip install -r requirements-analysis.txt

Write-Host `"Installing development dependencies...`" -ForegroundColor Green
pip install -r requirements-dev.txt

# Create environment configuration
@`"
# Environment Variables for Data Analysis
PYTHONPATH=src
DEBUG=True
JUPYTER_ENABLE_LAB=yes

# Data Paths
DATA_RAW_PATH=data/raw
DATA_PROCESSED_PATH=data/processed
DATA_OUTPUT_PATH=data/output

# Analysis Configuration
DEFAULT_SIGNIFICANCE_LEVEL=0.05
DEFAULT_CONFIDENCE_INTERVAL=0.95
MAX_MISSING_THRESHOLD=0.1

# Visualization Settings
MATPLOTLIB_BACKEND=Qt5Agg
PLOTLY_RENDERER=browser
DEFAULT_DPI=300
DEFAULT_FIGURE_SIZE=12,8

# Memory Management
PANDAS_MAX_COLUMNS=None
PANDAS_MAX_ROWS=None
NUMPY_RANDOM_SEED=42
`"@ | Out-File -FilePath `".env`" -Encoding utf8

# Create comprehensive .gitignore
@`"
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# Virtual Environment
.venv/
venv/
ENV/

# Jupyter Notebooks
.ipynb_checkpoints/
*/.ipynb_checkpoints/*

# Data Files (adjust based on your needs)
data/raw/*.sav
data/raw/*.xlsx
data/raw/*.csv
data/processed/*.parquet
data/output/*.pdf
data/output/*.png
data/output/*.html

# IDE
.vscode/
.idea/
*.swp
*.swo
*~

# Environment
.env
.env.local
.env.*.local

# Testing
.coverage
.pytest_cache/
htmlcov/

# Documentation
docs/_build/
site/

# OS
.DS_Store
Thumbs.db

# R (if using rpy2)
.Rhistory
.RData
.Ruserdata

# Temporary files
*.tmp
*.temp
*.log
`"@ | Out-File -FilePath `".gitignore`" -Encoding utf8

# Create pyproject.toml with data analysis configuration
@`"
[build-system]
requires = [`"setuptools>=45`", `"wheel`"]
build-backend = `"setuptools.build_meta`"

[project]
name = `"data-analysis-project`"
version = `"0.1.0`"
description = `"Advanced statistical data analysis project with SPSS support`"
requires-python = `">=3.9`"

[tool.black]
line-length = 88
target-version = ['py39']
include = '\.pyi?$'
extend-exclude = '''
/(
  \.eggs
  | \.git
  | \.venv
  | build
  | dist
)/
'''

[tool.isort]
profile = `"black`"
multi_line_output = 3
include_trailing_comma = true
force_grid_wrap = 0
use_parentheses = true
ensure_newline_before_comments = true
line_length = 88

[tool.pytest.ini_options]
testpaths = [`"tests`"]
python_files = [`"test_*.py`"]
python_classes = [`"Test*`"]
python_functions = [`"test_*`"]
addopts = `"--cov=src --cov-report=html --cov-report=term --cov-report=xml`"

[tool.mypy]
python_version = `"3.11`"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
ignore_missing_imports = true

[tool.pylint.messages_control]
disable = [`"C0330`", `"C0326`", `"R0903`", `"R0913`"]

[tool.coverage.run]
source = [`"src`"]
omit = [`"*/tests/*`", `"*/test_*`"]

[tool.coverage.report]
exclude_lines = [
    `"pragma: no cover`",
    `"def __repr__`",
    `"raise AssertionError`",
    `"raise NotImplementedError`"
]
`"@ | Out-File -FilePath `"pyproject.toml`" -Encoding utf8

Write-Host `"Environment setup complete!`" -ForegroundColor Green
Write-Host `"Jupyter Lab can be started with: jupyter lab`" -ForegroundColor Yellow
```

### Step 4: Global Data Analysis Declarative Memory Setup

**Create `.github/copilot-instructions.md`** with this exact content:

```markdown
# Project Catalyst - Data Analysis Cognitive Memory Architecture

IMPORTANT: This file serves as Global Data Analysis Declarative Memory. Keep minimal and efficient. Detailed execution resides in specialized memory files.

## ðŸ§  Enterprise Data Analysis Cognitive Architecture Status

**Working Memory**: 4/4 rules (at optimal capacity for enterprise data analysis and business intelligence)
**Consolidation**: Auto-trigger when exceeding capacity
**Memory Distribution**: Active across enterprise analytics procedural (.instructions.md) and business intelligence episodic (.prompt.md) systems

## ðŸš€ Enterprise Data Analysis Working Memory - Quick Reference (Limit: 4 Critical Rules)

| Priority | Rule | Load | Auto-Consolidate |
|----------|------|------|------------------|
| P1 | `@enterprise-analytics` - Apply enterprise-grade analytics with security, governance, and institutional compliance | High | Critical patterns |
| P2 | `@data-governance` - Ensure data quality, privacy compliance, and enterprise data governance standards | High | Governance updates |
| P3 | `@business-intelligence` - Deliver actionable business insights with automated reporting and stakeholder communication | High | Insight evolution |
| P4 | `@scalable-performance` - Optimize for big data processing, real-time analytics, and enterprise-scale performance | High | Performance patterns |

## ðŸŽ¯ Enterprise Data Analysis Cognitive Architecture Coordination

### Multi-Modal Enterprise Analytics Memory Distribution

**Procedural Memory Activation** (Context-Dependent):
- `data-analysis.instructions.md` â†’ Core data analysis patterns for .py, .ipynb files
- `enterprise-analytics.instructions.md` â†’ Enterprise analytics frameworks, governance, institutional standards
- `statistical-methods.instructions.md` â†’ Advanced statistical methods, hypothesis testing, experimental design
- `business-intelligence.instructions.md` â†’ BI frameworks, dashboard development, executive reporting
- `data-visualization.instructions.md` â†’ Enterprise visualization standards, interactive dashboards
- `machine-learning.instructions.md` â†’ ML/AI integration, model deployment, enterprise ML workflows
- `data-engineering.instructions.md` â†’ Data pipeline development, ETL processes, data architecture
- `etl-pipelines.instructions.md` â†’ Extract-Transform-Load automation, data integration workflows
- `data-governance.instructions.md` â†’ Data governance frameworks, quality standards, compliance
- `data-security.instructions.md` â†’ Data security protocols, encryption, access controls
- `privacy-compliance.instructions.md` â†’ GDPR, HIPAA, privacy regulations, compliance automation
- `quality-assurance.instructions.md` â†’ Data quality frameworks, validation, testing protocols
- `performance-optimization.instructions.md` â†’ Big data optimization, distributed computing, scalability
- `big-data.instructions.md` â†’ Big data technologies, distributed analytics, enterprise scale
- `cloud-analytics.instructions.md` â†’ Cloud analytics platforms, AWS, Azure, GCP integration
- `real-time-analytics.instructions.md` â†’ Streaming analytics, real-time dashboards, event processing
- `predictive-modeling.instructions.md` â†’ Predictive analytics, forecasting, scenario modeling
- `time-series.instructions.md` â†’ Time series analysis, forecasting, temporal patterns
- `experimental-design.instructions.md` â†’ A/B testing, experimental methodology, causal inference
- `hypothesis-testing.instructions.md` â†’ Statistical hypothesis testing, significance analysis
- `regression-analysis.instructions.md` â†’ Linear/nonlinear regression, model selection, validation
- `clustering-analysis.instructions.md` â†’ Clustering algorithms, segmentation, pattern recognition
- `classification.instructions.md` â†’ Classification models, supervised learning, performance metrics
- `dimensionality-reduction.instructions.md` â†’ PCA, factor analysis, feature selection
- `text-analytics.instructions.md` â†’ Natural language processing, sentiment analysis, text mining
- `network-analysis.instructions.md` â†’ Social network analysis, graph analytics, relationship modeling
- `geospatial-analysis.instructions.md` â†’ Geographic information systems, spatial analytics
- `survey-analysis.instructions.md` â†’ Survey methodology, questionnaire analysis, psychometrics
- `market-research.instructions.md` â†’ Market analysis, consumer insights, competitive intelligence
- `financial-analytics.instructions.md` â†’ Financial modeling, risk analysis, investment analytics
- `risk-analytics.instructions.md` â†’ Risk assessment, fraud detection, compliance monitoring
- `reporting-automation.instructions.md` â†’ Automated reporting, scheduled analytics, report generation
- `dashboard-development.instructions.md` â†’ Interactive dashboard creation, KPI monitoring
- `data-storytelling.instructions.md` â†’ Data narrative, executive communication, insight presentation
- `statistical-software.instructions.md` â†’ SPSS, R, SAS integration, software interoperability
- `database-analytics.instructions.md` â†’ SQL analytics, database optimization, data warehousing
- `api-integration.instructions.md` â†’ API data integration, web services, external data sources
- `model-deployment.instructions.md` â†’ Model operationalization, MLOps, production deployment
- `team-collaboration.instructions.md` â†’ Analytics team management, knowledge sharing
- `enterprise-standards.instructions.md` â†’ Enterprise analytics standards, best practices, frameworks

**Episodic Memory Activation** (Problem-Solving):
- `data-exploration.prompt.md` â†’ Comprehensive exploratory data analysis workflows
- `enterprise-reporting.prompt.md` â†’ Executive reporting and business intelligence workflows
- `statistical-analysis.prompt.md` â†’ Advanced statistical analysis and interpretation
- `business-insights.prompt.md` â†’ Business insight generation and strategic recommendations
- `data-cleaning.prompt.md` â†’ Enterprise data preprocessing and quality assurance
- `feature-engineering.prompt.md` â†’ Advanced feature engineering and data transformation
- `model-building.prompt.md` â†’ Predictive model development and optimization
- `model-validation.prompt.md` â†’ Model validation, testing, and performance assessment
- `performance-tuning.prompt.md` â†’ Analytics performance optimization and scalability
- `data-pipeline.prompt.md` â†’ Data pipeline architecture and implementation
- `etl-development.prompt.md` â†’ ETL process development and automation
- `dashboard-creation.prompt.md` â†’ Interactive dashboard design and development
- `visualization-design.prompt.md` â†’ Advanced data visualization and storytelling
- `anomaly-detection.prompt.md` â†’ Anomaly detection and outlier analysis
- `forecasting.prompt.md` â†’ Time series forecasting and predictive analytics
- `customer-analytics.prompt.md` â†’ Customer segmentation and lifetime value analysis
- `marketing-analytics.prompt.md` â†’ Marketing campaign analysis and optimization
- `sales-analytics.prompt.md` â†’ Sales performance analysis and forecasting
- `financial-modeling.prompt.md` â†’ Financial analysis and risk modeling
- `risk-assessment.prompt.md` â†’ Risk analysis and compliance monitoring
- `compliance-reporting.prompt.md` â†’ Regulatory compliance and audit reporting
- `data-governance.prompt.md` â†’ Data governance implementation and monitoring
- `quality-monitoring.prompt.md` â†’ Data quality monitoring and improvement
- `security-audit.prompt.md` â†’ Data security assessment and compliance validation
- `privacy-assessment.prompt.md` â†’ Privacy impact assessment and compliance
- `experimental-design.prompt.md` â†’ Experimental design and A/B testing workflows
- `ab-testing.prompt.md` â†’ A/B test design, execution, and analysis
- `survey-design.prompt.md` â†’ Survey methodology and questionnaire development
- `research-methodology.prompt.md` â†’ Research design and statistical methodology
- `data-integration.prompt.md` â†’ Data integration and multi-source analytics
- `database-optimization.prompt.md` â†’ Database performance and analytics optimization
- `cloud-migration.prompt.md` â†’ Cloud analytics migration and optimization
- `scalability-planning.prompt.md` â†’ Analytics scalability and architecture planning
- `automation-workflow.prompt.md` â†’ Analytics automation and workflow optimization
- `team-training.prompt.md` â†’ Analytics team training and skill development
- `project-planning.prompt.md` â†’ Analytics project management and planning
- `stakeholder-communication.prompt.md` â†’ Executive communication and insight presentation
- `enterprise-architecture.prompt.md` â†’ Enterprise analytics architecture design
- `consolidation.prompt.md` â†’ Memory consolidation and cognitive architecture optimization
- `self-assessment.prompt.md` â†’ Cognitive performance evaluation and improvement

### Enterprise Auto-Consolidation Triggers

- Working memory > 4 rules â†’ Execute consolidation.prompt.md
- Rule conflicts detected â†’ Activate enterprise-analytics.instructions.md
- Performance degradation â†’ Review and redistribute memory load with performance-tuning.prompt.md
- User requests meditation â†’ Full cognitive architecture optimization
- **Data quality issues detected â†’ Execute quality-monitoring.prompt.md**
- **Privacy compliance required â†’ Execute privacy-assessment.prompt.md**
- **Business insights needed â†’ Execute business-insights.prompt.md**
- **Model performance degradation â†’ Execute model-validation.prompt.md**
- **Dashboard updates required â†’ Execute dashboard-creation.prompt.md**
- **Stakeholder reporting â†’ Execute enterprise-reporting.prompt.md**
- **Data governance violations â†’ Execute data-governance.prompt.md**
- **Security audit required â†’ Execute security-audit.prompt.md**
- **Scalability planning â†’ Execute scalability-planning.prompt.md**
- **Team training needed â†’ Execute team-training.prompt.md**
- **Enterprise architecture review â†’ Execute enterprise-architecture.prompt.md**
- **Statistical analysis validation â†’ Execute statistical-analysis.prompt.md**
- **Meta-cognitive assessment needed â†’ Execute self-assessment.prompt.md**

## ðŸ§  Automatic Memory Consolidation Protocol

**Fundamental Cognitive Architecture Feature**: Project Catalyst implements automatic memory consolidation protocols that operate continuously without manual intervention, providing real-time cognitive architecture optimization for data analysis workflows.

### Trigger-Based Consolidation Activation

**Session Learning Accumulation**: When significant data analysis knowledge is gained during interaction sessions
**Explicit User Requests**: `"remember this`", `"commit to memory`", `"meditate`" commands trigger immediate consolidation
**Pattern Recognition**: When systematic analysis patterns, statistical methodologies, or data science techniques require procedural memory updates
**Cognitive Load Management**: When working memory approaches capacity limits (>4 rules)
**Complex Problem-Solving**: When breakthrough data analysis solutions require memory integration
**Workflow Optimization**: When data analysis process improvements enhance analytical efficiency
**Domain Expertise Expansion**: When new analysis patterns, statistical methodologies, or data science techniques require architectural updates
**Meta-Cognitive Insights**: When learning strategy optimizations improve data analysis performance

### Real-Time Consolidation Processing

**Global Memory Updates**: Update .github/copilot-instructions.md with data analysis session learnings
**Procedural Memory Enhancement**: Create/update .instructions.md files based on emerging data analysis patterns
**Episodic Memory Integration**: Create/update .prompt.md files for complex data analysis workflows
**Automatic Documentation**: Commit changes with descriptive consolidation messages for data analysis context
**Index Synchronization**: Update Long-Term Memory Index with new data analysis capabilities
**Cross-Reference Analysis**: Integrate new learnings with existing data analysis memory patterns
**Cognitive Load Optimization**: Distribute memory load for optimal data analysis performance

### Performance Integration

**Auto-Tracked Metrics**: Analysis effectiveness rates, insight quality improvements, data processing efficiency optimization
**Health Indicators**: Memory utilization patterns, activation frequency, data analysis workflow success rates
**Adaptive Thresholds**: Automatic adjustment based on data analysis performance metrics and analytical requirements

## ðŸ”„ Enterprise Memory Transfer Protocol

**Immediate Transfer**: Critical data quality and security issues â†’ Quick Reference (P1-P4)
**Gradual Consolidation**: Enterprise analytics patterns and frameworks â†’ Procedural memory (.instructions.md)
**Complex Workflows**: Multi-step business intelligence processes â†’ Episodic memory (.prompt.md)
**Archive Management**: Legacy analysis patterns â†’ Historical storage with version control
**Index Maintenance**: Auto-update Long-Term Memory Index with enterprise analytics enhancements
**Cross-Architecture Integration**: Sync with SETUP-PYTHON.md and other enterprise architectures for consistency

## ðŸ“š Enterprise Long-Term Memory Index

### Procedural Memory Store (.github/instructions/) - 45 Files
| File | Domain | Activation Pattern | Last Updated |
|------|--------|-------------------|--------------|
| data-analysis.instructions.md | Core Analysis | *.py, *.ipynb, data analysis | Auto-tracked |
| enterprise-analytics.instructions.md | Enterprise Analytics | Enterprise projects, BI | Auto-tracked |
| statistical-methods.instructions.md | Statistical Analysis | Hypothesis testing, experimental design | Auto-tracked |
| business-intelligence.instructions.md | Business Intelligence | BI frameworks, executive reporting | Auto-tracked |
| data-visualization.instructions.md | Enterprise Visualization | Dashboard development, storytelling | Auto-tracked |
| machine-learning.instructions.md | ML/AI Integration | Model deployment, MLOps | Auto-tracked |
| data-engineering.instructions.md | Data Engineering | Pipeline development, architecture | Auto-tracked |
| etl-pipelines.instructions.md | ETL Automation | Data integration, transformation | Auto-tracked |
| data-governance.instructions.md | Data Governance | Quality standards, compliance | Auto-tracked |
| data-security.instructions.md | Data Security | Encryption, access controls | Auto-tracked |
| privacy-compliance.instructions.md | Privacy Compliance | GDPR, HIPAA, regulations | Auto-tracked |
| quality-assurance.instructions.md | Quality Assurance | Validation, testing protocols | Auto-tracked |
| performance-optimization.instructions.md | Performance Optimization | Big data, distributed computing | Auto-tracked |
| big-data.instructions.md | Big Data Technologies | Distributed analytics, scale | Auto-tracked |
| cloud-analytics.instructions.md | Cloud Analytics | AWS, Azure, GCP integration | Auto-tracked |
| real-time-analytics.instructions.md | Real-time Analytics | Streaming, event processing | Auto-tracked |
| predictive-modeling.instructions.md | Predictive Analytics | Forecasting, scenario modeling | Auto-tracked |
| time-series.instructions.md | Time Series Analysis | Forecasting, temporal patterns | Auto-tracked |
| experimental-design.instructions.md | Experimental Design | A/B testing, causal inference | Auto-tracked |
| hypothesis-testing.instructions.md | Hypothesis Testing | Statistical significance | Auto-tracked |
| regression-analysis.instructions.md | Regression Analysis | Model selection, validation | Auto-tracked |
| clustering-analysis.instructions.md | Clustering Analysis | Segmentation, pattern recognition | Auto-tracked |
| classification.instructions.md | Classification Models | Supervised learning, metrics | Auto-tracked |
| dimensionality-reduction.instructions.md | Dimensionality Reduction | PCA, feature selection | Auto-tracked |
| text-analytics.instructions.md | Text Analytics | NLP, sentiment analysis | Auto-tracked |
| network-analysis.instructions.md | Network Analysis | Graph analytics, relationships | Auto-tracked |
| geospatial-analysis.instructions.md | Geospatial Analysis | GIS, spatial analytics | Auto-tracked |
| survey-analysis.instructions.md | Survey Analysis | Methodology, psychometrics | Auto-tracked |
| market-research.instructions.md | Market Research | Consumer insights, intelligence | Auto-tracked |
| financial-analytics.instructions.md | Financial Analytics | Financial modeling, risk | Auto-tracked |
| risk-analytics.instructions.md | Risk Analytics | Risk assessment, fraud detection | Auto-tracked |
| reporting-automation.instructions.md | Reporting Automation | Scheduled analytics, generation | Auto-tracked |
| dashboard-development.instructions.md | Dashboard Development | Interactive dashboards, KPIs | Auto-tracked |
| data-storytelling.instructions.md | Data Storytelling | Executive communication | Auto-tracked |
| statistical-software.instructions.md | Statistical Software | SPSS, R, SAS integration | Auto-tracked |
| database-analytics.instructions.md | Database Analytics | SQL, data warehousing | Auto-tracked |
| api-integration.instructions.md | API Integration | Web services, external data | Auto-tracked |
| model-deployment.instructions.md | Model Deployment | MLOps, production deployment | Auto-tracked |
| team-collaboration.instructions.md | Team Collaboration | Analytics team management | Auto-tracked |
| enterprise-standards.instructions.md | Enterprise Standards | Best practices, frameworks | Auto-tracked |
| cognitive-optimization.instructions.md | Cognitive Optimization | Memory optimization, performance | Auto-tracked |
| workflow-automation.instructions.md | Workflow Automation | Process automation, efficiency | Auto-tracked |
| data-lineage.instructions.md | Data Lineage | Data tracking, governance | Auto-tracked |
| model-monitoring.instructions.md | Model Monitoring | ML monitoring, performance | Auto-tracked |
| enterprise-integration.instructions.md | Enterprise Integration | System integration, architecture | Auto-tracked |

### Episodic Memory Store (.github/prompts/) - 45 Files
| File | Workflow Type | Complexity Level | Usage Frequency |
|------|---------------|------------------|-----------------|
| data-exploration.prompt.md | Exploratory Analysis | Medium | Auto-tracked |
| enterprise-reporting.prompt.md | Executive Reporting | High | Auto-tracked |
| statistical-analysis.prompt.md | Statistical Analysis | High | Auto-tracked |
| business-insights.prompt.md | Business Intelligence | High | Auto-tracked |
| data-cleaning.prompt.md | Data Preprocessing | Medium | Auto-tracked |
| feature-engineering.prompt.md | Feature Engineering | High | Auto-tracked |
| model-building.prompt.md | Model Development | High | Auto-tracked |
| model-validation.prompt.md | Model Validation | High | Auto-tracked |
| performance-tuning.prompt.md | Performance Optimization | High | Auto-tracked |
| data-pipeline.prompt.md | Data Pipeline Architecture | High | Auto-tracked |
| etl-development.prompt.md | ETL Development | High | Auto-tracked |
| dashboard-creation.prompt.md | Dashboard Creation | Medium | Auto-tracked |
| visualization-design.prompt.md | Visualization Design | Medium | Auto-tracked |
| anomaly-detection.prompt.md | Anomaly Detection | High | Auto-tracked |
| forecasting.prompt.md | Forecasting Analytics | High | Auto-tracked |
| customer-analytics.prompt.md | Customer Analytics | Medium | Auto-tracked |
| marketing-analytics.prompt.md | Marketing Analytics | Medium | Auto-tracked |
| sales-analytics.prompt.md | Sales Analytics | Medium | Auto-tracked |
| financial-modeling.prompt.md | Financial Modeling | High | Auto-tracked |
| risk-assessment.prompt.md | Risk Assessment | High | Auto-tracked |
| compliance-reporting.prompt.md | Compliance Reporting | High | Auto-tracked |
| data-governance.prompt.md | Data Governance | High | Auto-tracked |
| quality-monitoring.prompt.md | Quality Monitoring | Medium | Auto-tracked |
| security-audit.prompt.md | Security Audit | High | Auto-tracked |
| privacy-assessment.prompt.md | Privacy Assessment | High | Auto-tracked |
| experimental-design.prompt.md | Experimental Design | High | Auto-tracked |
| ab-testing.prompt.md | A/B Testing | Medium | Auto-tracked |
| survey-design.prompt.md | Survey Design | Medium | Auto-tracked |
| research-methodology.prompt.md | Research Methodology | High | Auto-tracked |
| data-integration.prompt.md | Data Integration | High | Auto-tracked |
| database-optimization.prompt.md | Database Optimization | High | Auto-tracked |
| cloud-migration.prompt.md | Cloud Migration | High | Auto-tracked |
| scalability-planning.prompt.md | Scalability Planning | High | Auto-tracked |
| automation-workflow.prompt.md | Automation Workflow | Medium | Auto-tracked |
| team-training.prompt.md | Team Training | Medium | Auto-tracked |
| project-planning.prompt.md | Project Planning | Medium | Auto-tracked |
| stakeholder-communication.prompt.md | Stakeholder Communication | High | Auto-tracked |
| enterprise-architecture.prompt.md | Enterprise Architecture | High | Auto-tracked |
| cognitive-assessment.prompt.md | Cognitive Assessment | High | Auto-tracked |
| workflow-optimization.prompt.md | Workflow Optimization | Medium | Auto-tracked |
| innovation-discovery.prompt.md | Innovation Discovery | Medium | Auto-tracked |
| strategic-planning.prompt.md | Strategic Planning | High | Auto-tracked |
| competitive-analysis.prompt.md | Competitive Analysis | Medium | Auto-tracked |
| consolidation.prompt.md | Memory Optimization | High | Auto-tracked |
| self-assessment.prompt.md | Self-Evaluation | High | Auto-tracked |

### Memory Transfer Protocol Status
- **Active Files**: 90 specialized memory files (45 procedural + 45 episodic)
- **Last Consolidation**: Enterprise enhancement with institutional-grade data analysis and business intelligence capabilities
- **Cognitive Load Status**: Optimized through distributed processing with enterprise analytics patterns
- **Index Synchronization**: Maintained automatically during consolidation with cross-architecture consistency
- **Meta-Cognitive Status**: Fully operational with enterprise-grade analytics assessment and business intelligence optimization
- **Enterprise Integration**: Synchronized with SETUP-PYTHON.md, SETUP-AZURE-SQL.md, and other enterprise architectures

---

*Global Declarative Memory Component - Coordinates distributed cognitive architecture while maintaining optimal working memory efficiency. Detailed execution protocols reside in specialized memory files.*
```

### Step 5: Procedural Memory Files

#### Create `.github/instructions/data-analysis.instructions.md`:

```markdown
---
applyTo: `"**/*.py,**/*.ipynb,**/data/**,**/notebooks/**`"
description: `"General data analysis patterns and best practices`"
---

# Data Analysis Procedural Memory

## Data Loading and Initial Inspection
- Use pandas.read_spss() or pyreadstat.read_sav() for SPSS files
- Perform initial data inspection with .info(), .describe(), .head()
- Check for missing values, duplicates, and data type consistency
- Document data source, collection method, and variable definitions
- Create data dictionaries for complex datasets

## Data Quality Assessment
- Identify and handle missing data patterns (MCAR, MAR, MNAR)
- Detect and address outliers using statistical methods
- Validate data ranges and logical consistency
- Check for multicollinearity in predictive variables
- Assess sample size adequacy for planned analyses

## Reproducible Analysis Workflow
- Use consistent naming conventions for variables and files
- Set random seeds for reproducible results
- Version control data processing scripts
- Document all analysis decisions and assumptions
- Create automated data processing pipelines

## Statistical Best Practices
- Choose appropriate statistical tests based on data type and distribution
- Check statistical assumptions before applying tests
- Use appropriate effect size measures alongside p-values
- Apply multiple comparison corrections when necessary
- Report confidence intervals and practical significance

## Code Organization and Documentation
- Structure notebooks with clear markdown sections
- Use functions for repeated analysis tasks
- Create reusable utility modules for common operations
- Include detailed comments explaining analysis rationale
- Generate automated reports with results and interpretations
```

#### Create `.github/instructions/statistical-modeling.instructions.md`:

```markdown
---
applyTo: `"**/*model*,**/*stat*,**/*test*,**/*regression*`"
description: `"Statistical modeling and hypothesis testing best practices`"
---

# Statistical Modeling Procedural Memory

## Hypothesis Testing Framework
- Clearly state null and alternative hypotheses
- Choose appropriate significance level (typically Î± = 0.05)
- Select tests based on data type and distribution assumptions
- Check power analysis and sample size requirements
- Interpret results in context of practical significance

## Descriptive Statistics
- Report appropriate measures of central tendency and variability
- Use robust statistics for non-normal distributions
- Include confidence intervals for point estimates
- Provide effect size measures (Cohen's d, eta-squared, etc.)
- Create comprehensive summary tables

## Regression Analysis
- Check linearity, independence, homoscedasticity, and normality assumptions
- Assess multicollinearity using VIF or correlation matrices
- Use appropriate model selection techniques (AIC, BIC, cross-validation)
- Validate models using train/test splits or cross-validation
- Interpret coefficients in context of the research question

## Multivariate Analysis
- Apply appropriate dimensionality reduction techniques (PCA, FA)
- Use clustering methods suitable for data structure
- Validate cluster solutions using multiple criteria
- Interpret factor loadings and component meanings
- Report model fit statistics and validation metrics

## Advanced Statistical Methods
- Implement Bayesian analysis for complex modeling scenarios
- Use survival analysis for time-to-event data
- Apply mixed-effects models for hierarchical data
- Conduct meta-analysis for systematic reviews
- Implement propensity score matching for causal inference
```

#### Create `.github/instructions/spss-processing.instructions.md`:

```markdown
---
applyTo: `"**/*.sav,**/*spss*,**/*pyreadstat*`"
description: `"SPSS data file processing and migration patterns`"
---

# SPSS Processing Procedural Memory

## SPSS File Reading
- Use pyreadstat.read_sav() for comprehensive metadata preservation
- Extract variable labels, value labels, and missing value definitions
- Preserve SPSS data types and measurement levels
- Handle SPSS system missing values appropriately
- Document original SPSS variable names and transformations

## Data Type Conversion
- Convert SPSS string variables to appropriate pandas dtypes
- Transform SPSS date/time variables to pandas datetime
- Handle SPSS categorical variables with proper encoding
- Preserve ordinal variable ordering from SPSS
- Convert SPSS missing value codes to pandas NaN

## Variable Management
- Create comprehensive variable dictionaries
- Map SPSS variable labels to descriptive column names
- Preserve value labels for categorical variables
- Document any variable transformations or recodings
- Maintain backward compatibility with original SPSS syntax

## Analysis Migration
- Translate common SPSS procedures to Python equivalents
- Implement SPSS syntax patterns using pandas and scipy
- Reproduce SPSS output formats for comparison
- Validate migrated analyses against original SPSS results
- Document differences in calculation methods or assumptions

## Metadata Preservation
- Export enhanced datasets with full documentation
- Create SPSS-compatible output for collaboration
- Generate syntax files for reproducible SPSS workflows
- Maintain audit trails for data modifications
- Provide clear migration documentation for stakeholders
```

#### Create `.github/instructions/visualization.instructions.md`:

```markdown
---
applyTo: `"**/*plot*,**/*chart*,**/*visual*,**/*graph*`"
description: `"Data visualization design and implementation best practices`"
---

# Visualization Procedural Memory

## Chart Selection and Design
- Choose appropriate chart types based on data structure and message
- Follow principles of visual hierarchy and cognitive load reduction
- Use consistent color schemes and typography throughout projects
- Ensure accessibility with colorblind-friendly palettes
- Apply appropriate aspect ratios and scaling

## Statistical Visualization
- Include error bars, confidence intervals, and uncertainty measures
- Use appropriate binning and smoothing for distributions
- Show individual data points when sample sizes are small
- Apply statistical overlays (regression lines, confidence bands)
- Annotate significant differences and effect sizes

## Interactive and Dynamic Plots
- Implement interactive features that enhance understanding
- Use tooltips and hover information effectively
- Create linked plots for multi-dimensional exploration
- Design responsive layouts for different screen sizes
- Optimize performance for large datasets

## Publication-Quality Graphics
- Use vector formats (SVG, PDF) for scalable graphics
- Apply consistent styling with custom themes
- Include comprehensive titles, labels, and captions
- Follow journal-specific formatting requirements
- Create automated figure generation pipelines

## Dashboard and Report Integration
- Design cohesive visual narratives across multiple charts
- Implement effective layout and spacing principles
- Use consistent data-ink ratios and minimize chartjunk
- Create executive summary visualizations
- Enable easy updating with new data
```

#### Create `.github/instructions/jupyter.instructions.md`:

```markdown
---
applyTo: `"**/*.ipynb,**/notebooks/**`"
description: `"Jupyter notebook organization and documentation standards`"
---

# Jupyter Notebook Procedural Memory

## Notebook Structure and Organization
- Start with clear title, author, and date information
- Include table of contents for longer notebooks
- Use hierarchical markdown headers for logical sections
- Separate exploration, analysis, and reporting sections
- End with summary and conclusions

## Code Cell Best Practices
- Keep cells focused on single tasks or concepts
- Use descriptive variable names and consistent styling
- Include inline comments for complex operations
- Clear outputs before committing to version control
- Use markdown cells to explain analysis rationale

## Data and Results Documentation
- Document data sources, collection methods, and limitations
- Explain statistical methods and assumption checking
- Interpret results in context of research questions
- Include methodology references and citations
- Provide clear conclusions and recommendations

## Reproducibility and Version Control
- Set random seeds for stochastic processes
- Pin package versions in requirements files
- Use relative paths for data and output files
- Create automated notebook execution pipelines
- Generate static reports for sharing and archiving

## Interactive Elements and Widgets
- Use ipywidgets for parameter exploration
- Implement interactive plotting with plotly or bokeh
- Create dynamic filtering and selection interfaces
- Design user-friendly parameter adjustment tools
- Enable real-time visualization updates
```

### Step 6: Episodic Memory Files

#### Create `.github/prompts/exploratory-analysis.prompt.md`:

```markdown
---
mode: `"agent`"
model: `"gpt-4`"
tools: [`"workspace`", `"run_in_terminal`", `"read_file`", `"create_file`"]
description: `"Systematic exploratory data analysis workflow`"
---

# Exploratory Data Analysis Episode Template

## Phase 1: Data Import and Initial Assessment
- Load SPSS .sav files using pyreadstat with metadata preservation
- Examine dataset structure, dimensions, and variable types
- Review variable labels, value labels, and missing value patterns
- Create comprehensive data dictionary and codebook
- Document data source and collection methodology

## Phase 2: Univariate Analysis
- Generate descriptive statistics for all variables
- Create appropriate visualizations (histograms, box plots, bar charts)
- Assess distributions and identify potential transformations
- Detect outliers using statistical methods (IQR, z-scores)
- Document unusual patterns or data quality issues

## Phase 3: Bivariate and Multivariate Exploration
- Create correlation matrices and heatmaps
- Generate cross-tabulations for categorical variables
- Explore relationships using scatter plots and regression lines
- Identify potential confounding variables
- Assess patterns of missing data across variables

## Phase 4: Hypothesis Generation and Analysis Planning
- Formulate research questions based on data exploration
- Identify appropriate statistical tests and modeling approaches
- Plan additional data collection or transformation needs
- Document assumptions and limitations
- Create analysis roadmap for subsequent investigations

Use sophisticated analytics tools from ${workspaceFolder}/.venv
```

#### Create `.github/prompts/multivariate-analysis.prompt.md`:

```markdown
---
mode: `"agent`"
model: `"gpt-4`"
tools: [`"workspace`", `"run_in_terminal`", `"read_file`", `"create_file`"]
description: `"Comprehensive multivariate statistical analysis workflow`"
---

# Multivariate Analysis Episode Template

## Phase 1: Data Preparation and Assumption Checking
- Assess multivariate normality using Mardia's test
- Check for multicollinearity using VIF and correlation matrices
- Handle missing data using appropriate imputation methods
- Standardize variables when necessary for analysis
- Verify sample size adequacy using power analysis

## Phase 2: Dimensionality Reduction
- Apply Principal Component Analysis (PCA) with scree plots
- Implement Factor Analysis with appropriate rotation methods
- Use parallel analysis to determine optimal number of factors
- Interpret factor loadings and component meanings
- Validate factor structure using confirmatory methods

## Phase 3: Clustering and Classification
- Apply hierarchical clustering with dendrogram analysis
- Implement k-means clustering with optimal cluster determination
- Use DBSCAN for density-based clustering when appropriate
- Validate cluster solutions using silhouette analysis
- Profile clusters using discriminant analysis

## Phase 4: Advanced Multivariate Methods
- Conduct MANOVA for multiple dependent variables
- Apply canonical correlation analysis for relationship exploration
- Implement structural equation modeling when appropriate
- Use machine learning methods for prediction and classification
- Validate models using cross-validation and holdout samples

Use advanced statistical libraries from ${workspaceFolder}/.venv
```

#### Create `.github/prompts/spss-migration.prompt.md`:

```markdown
---
mode: `"agent`"
model: `"gpt-4`"
tools: [`"workspace`", `"run_in_terminal`", `"read_file`", `"create_file`"]
description: `"SPSS to Python analysis migration workflow`"
---

# SPSS Migration Episode Template

## Phase 1: SPSS File Analysis and Import
- Load .sav files using pyreadstat with complete metadata
- Extract and document variable labels, value labels, and formats
- Identify SPSS system missing values and user-defined missing values
- Preserve measurement levels (nominal, ordinal, scale)
- Create comprehensive variable mapping documentation

## Phase 2: Syntax Translation and Validation
- Translate SPSS FREQUENCIES to pandas value_counts and crosstabs
- Convert SPSS DESCRIPTIVES to pandas describe and scipy stats
- Migrate SPSS T-TEST to scipy.stats t-tests and effect sizes
- Transform SPSS ANOVA to statsmodels ANOVA and post-hoc tests
- Reproduce SPSS REGRESSION using statsmodels OLS and diagnostics

## Phase 3: Output Comparison and Validation
- Generate side-by-side comparisons of SPSS and Python results
- Validate statistical test results within acceptable tolerance
- Document any differences in calculation methods or assumptions
- Create equivalent visualization outputs using matplotlib/seaborn
- Ensure p-values, test statistics, and effect sizes match

## Phase 4: Enhanced Python Implementation
- Extend analyses with additional diagnostics not available in SPSS
- Implement advanced visualization capabilities
- Add robust statistical methods and bootstrap confidence intervals
- Create interactive dashboards for ongoing analysis
- Document advantages and enhanced capabilities of Python implementation

Use SPSS processing tools from ${workspaceFolder}/.venv
```

#### Create additional episodic memory files:

```markdown
# Additional episodic memory files to create:
# - .github/prompts/descriptive-statistics.prompt.md
# - .github/prompts/inferential-statistics.prompt.md
# - .github/prompts/time-series-analysis.prompt.md
# - .github/prompts/machine-learning.prompt.md
# - .github/prompts/data-cleaning.prompt.md
# - .github/prompts/visualization-creation.prompt.md
# - .github/prompts/report-generation.prompt.md
# - .github/prompts/consolidation.prompt.md (same as other setups)
# - .github/prompts/self-assessment.prompt.md (same as other setups)
# - .github/prompts/meta-learning.prompt.md (same as other setups)
# - .github/prompts/cognitive-health.prompt.md (same as other setups)
```

## ðŸŽ¯ Setup Validation

After creating all files, verify setup:

1. **Check virtual environment**: Ensure `.venv` is created with all analytics packages
2. **Test SPSS file reading**: Verify pyreadstat can read .sav files
3. **Validate Jupyter setup**: Confirm Jupyter Lab/Notebook can access .venv kernel
4. **Check statistical libraries**: Test imports for statsmodels, scipy, sklearn
5. **Verify visualization tools**: Confirm matplotlib, seaborn, plotly work correctly

## ðŸš€ Quick Start Commands

After setup, test with these commands in GitHub Copilot:

**SPSS Integration Tests**:
- `@workspace Load and analyze this .sav file` (Should activate spss-processing.instructions.md)
- `Help me migrate this SPSS analysis to Python` (Should activate spss-migration.prompt.md)

**Statistical Analysis Tests**:
- `Perform comprehensive exploratory data analysis` (Should activate exploratory-analysis.prompt.md)
- `Conduct multivariate statistical analysis` (Should activate multivariate-analysis.prompt.md)
- `Create publication-quality visualizations` (Should activate visualization-creation.prompt.md)

**Advanced Analytics Tests**:
- `Build predictive models for this dataset` (Should activate machine-learning.prompt.md)
- `Analyze time series patterns and trends` (Should activate time-series-analysis.prompt.md)

**Meta-Cognition Tests**:
- `@workspace Assess your statistical analysis performance` (Should activate self-assessment.prompt.md)
- `How can you improve your data analysis strategies?` (Should activate meta-learning.prompt.md)

## âš¡ Success Indicators

Your data analysis cognitive architecture is working when:
- SPSS .sav files load seamlessly with preserved metadata
- Statistical analyses follow appropriate methodological standards
- Visualizations are publication-quality and follow best practices
- Jupyter notebooks are well-organized and reproducible
- All analyses include proper assumption checking and validation
- **Meta-cognitive capabilities**: The AI can assess its statistical analysis performance
- **Self-monitoring**: The system tracks analysis patterns and suggests improvements
- **Learning evolution**: The AI improves its statistical methodology over time
- **Methodological awareness**: The system validates assumptions and suggests appropriate tests

## ðŸ”„ Maintenance

- Update statistical packages regularly with careful version testing
- Review and update statistical methodology based on latest research
- Execute consolidation meditation when adding 5+ new analysis patterns
- Monitor computational performance with large datasets
- **Execute self-assessment weekly to monitor analysis quality**
- **Run meta-learning analysis monthly for methodology optimization**
- **Perform cognitive architecture health checks quarterly**
- **Update statistical capabilities based on new research and methods**

## ðŸ“– Recommended Learning Resources

- **Statistical Methods**: `"An Introduction to Statistical Learning`" by James et al.
- **Python for Data Science**: `"Python Data Science Handbook`" by VanderPlas
- **Advanced Statistics**: `"The Elements of Statistical Learning`" by Hastie et al.
- **SPSS Migration**: Official SPSS Python integration documentation
- **Visualization**: `"Fundamentals of Data Visualization`" by Wilke

---


---

## ðŸ†˜ Meta-Cognition Help System

**Need help with cognitive architecture features?** The smart help system can assist you with:

### ðŸ§  **Cognitive Commands**
- ``"help`"` - Get assistance with meta-cognitive features
- ``"meditate`"` - Optimize cognitive architecture and consolidate learnings
- ``"remember [insight]`"` - Commit important learning to memory files
- ``"show memory status`"` - Display current cognitive load and architecture health

### ðŸŽ¯ **Professional Architecture Support**
- **Memory Management**: Working memory optimization and consolidation protocols
- **Learning Enhancement**: Adaptive learning and error prevention systems
- **Performance Monitoring**: Self-assessment and continuous improvement
- **Domain Integration**: Seamless integration with data analysis workflows

### ðŸ”„ **Smart Help Routing**
**Meta-Cognitive Questions** (Specialized Help):
- Questions about memory, meditation, consolidation, cognitive architecture
- Help with Project Catalyst features and commands
- Performance optimization and learning strategies

**Technical Questions** (Standard Copilot):
- Domain-specific statistical analysis, coding, implementation, and troubleshooting
- data analysis best practices and methodologies
- Project setup, configuration, and deployment

### ðŸ’¡ **Getting Started**
1. Try asking: ``"help with meditation`"` to learn about cognitive optimization
2. Use ``"remember [insight]`"` to consolidate important learnings from your data analysis work
3. Say ``"meditate`"` when you want to optimize your cognitive architecture

The help system automatically detects whether you need meta-cognitive assistance or technical support and routes your request appropriately.

**Ready to enhance your data analysis capabilities with meta-cognitive AI?** Just ask for help!
---

## ðŸ¢ Enterprise Framework Integration

**Enhanced with 9-Point Enterprise Methodology** for data analysis excellence:

### ðŸ›¡ï¸ **Enterprise-Grade Data Analysis**
- **Security**: Information protection protocols and data analysis security safeguards
- **Testing**: Data analysis validation and methodology testing protocols
- **Automation**: Data analysis process automation and workflow optimization
- **Risk**: Data analysis risk assessment and mitigation strategies
- **Performance**: Data analysis performance optimization and efficiency management
- **Quality**: Data analysis excellence standards and quality assurance protocols
- **Documentation**: Comprehensive data analysis documentation and record keeping
- **Analytics**: Data analysis performance metrics and analytical insights

### ðŸŽ¯ **Enterprise Commands**
- ``"apply enterprise methodology`"` - Implement 9-point framework for current data analysis project
- ``"validate data analysis approach`"` - Apply enterprise testing and quality protocols
- ``"assess data analysis risks`"` - Evaluate methodology and execution risks
- ``"optimize data analysis performance`"` - Apply enterprise performance optimization

### ðŸ”„ **Integrated Cognitive-Enterprise Workflow**
**Meta-Cognitive + Enterprise**: The cognitive architecture automatically applies enterprise methodology principles during data analysis tasks, ensuring both adaptive learning AND systematic excellence.



## ðŸ“„ Project Documentation Setup

**Step 15: Create Project README.md**

Execute the following to create a comprehensive project README file with banner image and proper documentation links:

`powershell
@"
![Data Analysis Cognitive Architecture](banner2.png)

# Data Analysis & Professional Excellence Cognitive Architecture

[![Data_Analysis](https://img.shields.io/badge/Data_Analysis-blue?style=for-the-badge&logo=chart-bar&logoColor=white)](#) [![Meta Cognitive](https://img.shields.io/badge/Meta-Cognitive-red?style=for-the-badge&logo=psychology)](#) [![Dual Context](https://img.shields.io/badge/Dual_Context-Framework_Integrated-orange?style=for-the-badge&logo=microsoft&logoColor=white)](#) [![Enterprise Framework](https://img.shields.io/badge/9_Point-Framework-gold?style=for-the-badge&logo=shield)](#)

## ðŸŽ¯ Project Overview

This project implements the **Project Catalyst Enterprise Data Analysis Cognitive Architecture** - an advanced AI-powered system for Data Analysis excellence and professional development. The cognitive architecture combines human memory models with enterprise-grade methodologies to provide intelligent assistance for professional Data Analysis applications and organizational excellence.

## ðŸ§  Cognitive Architecture Features

### Meta-Cognitive Capabilities
- **Adaptive Learning**: Self-improving system that learns from your Data Analysis patterns and professional preferences
- **Memory Management**: Intelligent consolidation of Data Analysis knowledge, methodologies, and best practices
- **Context Awareness**: Dual-context support for Microsoft internal and universal organizational environments
- **Performance Monitoring**: Self-assessment and continuous improvement protocols for Data Analysis excellence

### Enterprise Integration
- **9-Point Framework**: Security, Testing, Automation, Risk, Performance, Quality, Documentation, Analytics, Compliance
- **Dual-Context Deployment**: Automatic adaptation for Microsoft internal or external organizational requirements
- **Professional Validation**: Research-backed methodologies with empirical integrity and quality protocols

## ðŸš€ Getting Started

### Prerequisites
- Visual Studio Code with GitHub Copilot extension
- Basic understanding of Data Analysis concepts and methodologies
- Professional context requiring Data Analysis excellence
- Understanding of enterprise quality standards

### Quick Start
1. **Activate the Cognitive Architecture**: 
   `
   "show memory status"
   `

2. **Apply Enterprise Framework**:
   `
   "apply enterprise methodology"
   `

3. **Begin Data Analysis Excellence**:
   `
   "help me develop a Data Analysis strategy for [your specific context]"
   `

## ðŸ“š Documentation

### Core Documentation
- **[Data Analysis Manual](MANUAL-DATA-ANALYSIS.md)** - Comprehensive user guide with commands, examples, and best practices
- **[Setup Guide](SETUP-DATA-ANALYSIS.md)** - Complete installation and configuration instructions
- **[Enterprise Framework](IMPLEMENTATION_GUIDE.md)** - 9-point enterprise methodology implementation

### Professional Architectures
- **[Leadership](SETUP-LEADERSHIP.md)** - Executive leadership and organizational transformation
- **[Meta-Meta-Cognition](SETUP-META-META-COGNITION.md)** - Universal cognitive architecture generator
- **[Academic Research](SETUP-ACADEMIC.md)** - Research methodology and validation frameworks

## ðŸ¢ Enterprise Benefits

- **Professional Excellence**: AI-powered assistance enhances Data Analysis quality and methodology effectiveness
- **Best Practices**: Enterprise-grade frameworks ensure systematic and scalable Data Analysis approaches
- **Knowledge Management**: Cognitive architecture captures and organizes Data Analysis insights and methodologies
- **Quality Assurance**: Advanced validation and compliance frameworks maintain professional standards
- **Continuous Improvement**: Systematic approach to Data Analysis optimization and professional development

## ðŸš€ Advanced Features

- **Meta-Cognitive Learning**: System evolves based on your Data Analysis patterns and professional requirements
- **Context Preservation**: Maintains Data Analysis context and insights across professional development sessions
- **Enterprise Validation**: Automatic quality and consistency checking for professional standards
- **Performance Optimization**: Continuous improvement protocols for Data Analysis and professional excellence

---

**Ready to transform your Data Analysis capabilities with enterprise-grade cognitive AI assistance?**

Explore the [Data Analysis Manual](MANUAL-DATA-ANALYSIS.md) to unlock the full potential of your Data Analysis cognitive architecture system.

*Project Catalyst - Enterprise Data Analysis Cognitive Architecture v0.9.0*
"@ | Out-File -FilePath "README.md" -Encoding UTF8
`

**âœ… README.md Created**: Your project now includes comprehensive Data Analysis documentation with banner image, professional capabilities overview, and proper manual linking for immediate team orientation and guidance.

---

---

**DATA ANALYSIS SETUP COMPLETE**: Your adaptive AI statistical analysis partner is now ready with comprehensive SPSS .sav file processing, sophisticated analytical tools, publication-quality visualizations, and meta-cognitive self-monitoring for advanced data science workflows.



